





# Verify installations
import importlib

packages = [
    ('google.adk', 'google-adk'),
    ('google.cloud.aiplatform', 'google-cloud-aiplatform'),
    ('vertexai', 'vertexai'),
    ('pandas', 'pandas'),
    ('numpy', 'numpy'),
    ('sklearn', 'scikit-learn'),
    ('lifelines', 'lifelines')
]

for pkg, name in packages:
    try:
        mod = importlib.import_module(pkg)
        version = getattr(mod, '__version__', 'unknown')
        print(f"âœ… {name}: {version}")
    except ImportError as e:
        print(f"âŒ {name}: {e}")


# Google Cloud Authentication for Local Notebook
import os
import google.auth

# Set up Cloud Credentials Locally
# This relies on the 'gcloud auth application-default login' command run previously
try:
    credentials, project = google.auth.default()
    print("âœ… Cloud credentials configured")
except Exception as e:
    print(f"âŒ Error loading credentials: {e}")
    print("Please run 'gcloud auth application-default login' in your terminal.")

# Google Cloud Configuration
PROJECT_ID = "sunlit-gamma-342416"
REGION = "us-east4"  # Vertex AI Agent Engine region
BUCKET_NAME = f"{PROJECT_ID}-churn-agents"  # GCS bucket for artifacts

# Model Configuration
LLM_MODEL = "gemini-2.0-flash"  # Cost-efficient model
EMBEDDING_MODEL = "text-embedding-004"

# Agent Configuration
AGENT_NAME = "churn_prevention_agent"
AGENT_DISPLAY_NAME = "Proactive Churn Prevention System"

# Set environment variables
# Note: Explicitly setting GOOGLE_CLOUD_PROJECT is good practice locally
os.environ["GOOGLE_CLOUD_PROJECT"] = PROJECT_ID
os.environ["GOOGLE_CLOUD_REGION"] = REGION

print(f"Project: {PROJECT_ID}")
print(f"Region: {REGION}")
print(f"Model: {LLM_MODEL}")





import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
from typing import Dict, Any, List, Optional

# Set random seed for reproducibility
np.random.seed(11)

def generate_customer_data(n_customers: int = 1000) -> pd.DataFrame:
    """
    Generate synthetic customer churn dataset with behavioral signals.
    """
    # Customer IDs
    customer_ids = [f"CUST_{i:06d}" for i in range(1, n_customers + 1)]
    
    # Demographics
    tenure_months = np.random.exponential(scale=24, size=n_customers).astype(int)
    tenure_months = np.clip(tenure_months, 1, 120)
    
    subscription_tiers = np.random.choice(
        ['Basic', 'Standard', 'Premium', 'Enterprise'],
        size=n_customers,
        p=[0.30, 0.35, 0.25, 0.10]
    )
    
    tier_charges = {'Basic': 29, 'Standard': 79, 'Premium': 149, 'Enterprise': 299}
    monthly_charges = [tier_charges[tier] * np.random.uniform(0.9, 1.1) for tier in subscription_tiers]
    
    # Behavioral signals
    login_frequency = np.random.poisson(lam=15, size=n_customers)
    feature_usage_pct = np.random.beta(a=2, b=5, size=n_customers) * 100
    support_tickets_90d = np.random.poisson(lam=2, size=n_customers)
    
    # Financial indicators
    payment_delays_12m = np.random.poisson(lam=0.5, size=n_customers)
    discount_count = np.random.poisson(lam=1, size=n_customers)
    
    # Engagement metrics
    nps_score = np.random.choice(range(0, 11), size=n_customers, p=[
        0.02, 0.02, 0.03, 0.05, 0.08, 0.10, 0.15, 0.20, 0.18, 0.12, 0.05
    ])
    email_open_rate = np.random.beta(a=3, b=4, size=n_customers)
    last_activity_days = np.random.exponential(scale=7, size=n_customers).astype(int)
    
    # Generate churn labels
    churn_score = (
        -0.02 * tenure_months +
        -0.03 * login_frequency +
        -0.02 * feature_usage_pct +
        0.15 * support_tickets_90d +
        0.25 * payment_delays_12m +
        -0.1 * nps_score +
        0.02 * last_activity_days +
        np.random.normal(0, 0.5, n_customers)
    )
    
    churn_probability = 1 / (1 + np.exp(-churn_score))
    churned = np.random.binomial(1, churn_probability)
    
    days_until_churn = np.where(
        churned == 1,
        np.random.exponential(scale=45, size=n_customers).astype(int),
        np.nan
    )
    
    df = pd.DataFrame({
        'customer_id': customer_ids,
        'tenure_months': tenure_months,
        'subscription_tier': subscription_tiers,
        'monthly_charges': np.round(monthly_charges, 2),
        'login_frequency_monthly': login_frequency,
        'feature_usage_pct': np.round(feature_usage_pct, 1),
        'support_tickets_90d': support_tickets_90d,
        'payment_delays_12m': payment_delays_12m,
        'discount_count': discount_count,
        'nps_score': nps_score,
        'email_open_rate': np.round(email_open_rate, 3),
        'last_activity_days': last_activity_days,
        'churned': churned,
        'days_until_churn': days_until_churn,
        'churn_probability': np.round(churn_probability, 4)
    })
    
    return df

# Generate dataset
customer_df = generate_customer_data(n_customers=1600)

print(f"Dataset Shape: {customer_df.shape}")
print(f"Churn Rate: {customer_df['churned'].mean():.1%}")
print(f"\nSample Records:")
customer_df.head()


# Feature Engineering

def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    
    # Engagement Score
    df['engagement_score'] = (
        0.3 * (df['login_frequency_monthly'] / df['login_frequency_monthly'].max()) +
        0.3 * (df['feature_usage_pct'] / 100) +
        0.2 * df['email_open_rate'] +
        0.2 * (df['nps_score'] / 10)
    ) * 100
    
    # Risk Indicators
    df['is_high_value'] = (df['subscription_tier'].isin(['Premium', 'Enterprise'])).astype(int)
    df['has_payment_issues'] = (df['payment_delays_12m'] > 0).astype(int)
    df['is_heavy_support_user'] = (df['support_tickets_90d'] >= 3).astype(int)
    df['is_inactive'] = (df['last_activity_days'] > 14).astype(int)
    
    # CLV
    df['clv_estimate'] = df['monthly_charges'] * df['tenure_months'] * 0.8
    
    # Risk Score
    df['risk_score_baseline'] = (
        25 * df['has_payment_issues'] +
        20 * df['is_heavy_support_user'] +
        15 * df['is_inactive'] +
        10 * (df['nps_score'] < 7).astype(int) +
        30 * (1 - df['engagement_score'] / 100)
    )
    df['risk_score_baseline'] = np.clip(df['risk_score_baseline'], 0, 100)
    
    return df

customer_df = engineer_features(customer_df)
print("âœ… Feature engineering complete")
print(customer_df[['customer_id', 'engagement_score', 'risk_score_baseline', 'clv_estimate']].head())


# Train/Test Split
from sklearn.model_selection import train_test_split

feature_cols = [
    'tenure_months', 'monthly_charges', 'login_frequency_monthly',
    'feature_usage_pct', 'support_tickets_90d', 'payment_delays_12m',
    'discount_count', 'nps_score', 'email_open_rate', 'last_activity_days',
    'engagement_score', 'is_high_value', 'has_payment_issues',
    'is_heavy_support_user', 'is_inactive'
]

X = customer_df[feature_cols]
y = customer_df['churned']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=11, stratify=y
)

print(f"Training Set: {len(X_train)} customers ({y_train.mean():.1%} churn rate)")
print(f"Test Set: {len(X_test)} customers ({y_test.mean():.1%} churn rate)")


# Save dataset
customer_df.to_csv('/tmp/customer_churn_data.csv', index=False)
print("âœ… Dataset saved to /tmp/customer_churn_data.csv")





# ============================================================
# CORE IMPORTS FOR ADK v1.0.0+
# ============================================================

from google.adk.agents import (
    Agent,              # Main agent class (replaces LlmAgent)
    SequentialAgent,    # Runs sub-agents in sequence
    ParallelAgent,      # Runs sub-agents in parallel
    LoopAgent           # Runs sub-agents in a loop
)
from google.adk.code_executors import BuiltInCodeExecutor  
from google.adk.sessions import InMemorySessionService
from google.adk.runners import Runner
from google.genai import types as genai_types
import logging

# For async support in notebooks
import nest_asyncio
nest_asyncio.apply()
import asyncio

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger('ChurnPreventionAgents')

print("âœ… ADK v1.0.0+ imports successful")


# ============================================================
# CUSTOM TOOL FUNCTIONS
# In ADK v1.0.0+, pass functions directly to agents
# The function name becomes the tool name
# The docstring becomes the description
# ============================================================

def calculate_churn_score(customer_id: str) -> Dict[str, Any]:
    """
    Calculate churn risk score for a specific customer using behavioral signals.
    
    Args:
        customer_id: Unique customer identifier (e.g., CUST_000001)
        
    Returns:
        Dictionary with churn_probability, risk_tier, key_risk_factors, and confidence
    """
    logger.info(f"Calculating churn score for {customer_id}")
    
    df = pd.read_csv('/tmp/customer_churn_data.csv')
    customer = df[df['customer_id'] == customer_id]
    
    if customer.empty:
        return {"error": f"Customer {customer_id} not found"}
    
    c = customer.iloc[0]
    
    # Calculate churn probability
    base_risk = c['risk_score_baseline'] / 100
    engagement_factor = 1 - (c['engagement_score'] / 100)
    churn_prob = np.clip(
        0.3 * base_risk + 0.4 * engagement_factor + 0.3 * c['churn_probability'],
        0, 1
    )
    
    # Determine risk tier
    if churn_prob >= 0.75:
        risk_tier = "Critical"
    elif churn_prob >= 0.50:
        risk_tier = "High"
    elif churn_prob >= 0.25:
        risk_tier = "Medium"
    else:
        risk_tier = "Low"
    
    # Identify risk factors
    risk_factors = []
    if c['has_payment_issues']:
        risk_factors.append("Payment issues detected")
    if c['is_heavy_support_user']:
        risk_factors.append("High support ticket volume")
    if c['is_inactive']:
        risk_factors.append("Product inactivity")
    if c['nps_score'] < 7:
        risk_factors.append("Low satisfaction score")
    if c['engagement_score'] < 30:
        risk_factors.append("Low engagement")
    
    days_estimate = int(60 * (1 - churn_prob)) if churn_prob > 0.3 else None
    
    return {
        "customer_id": customer_id,
        "churn_probability": round(float(churn_prob), 4),
        "risk_tier": risk_tier,
        "days_until_churn": days_estimate,
        "confidence": 0.85,
        "key_risk_factors": risk_factors if risk_factors else ["No major risk factors"]
    }


def recommend_intervention(customer_id: str, churn_probability: float, risk_factors: List[str]) -> Dict[str, Any]:
    """
    Generate personalized retention intervention based on churn prediction.
    
    Args:
        customer_id: Customer identifier
        churn_probability: Predicted churn probability (0-1)
        risk_factors: List of identified risk factors
        
    Returns:
        Dictionary with intervention_type, priority, expected_lift, roi_estimate
    """
    logger.info(f"Generating intervention for {customer_id}")
    
    df = pd.read_csv('/tmp/customer_churn_data.csv')
    customer = df[df['customer_id'] == customer_id]
    
    if customer.empty:
        return {"error": f"Customer {customer_id} not found"}
    
    c = customer.iloc[0]
    clv = c['clv_estimate']
    tier = c['subscription_tier']
    
    interventions = {
        "Payment issues detected": {"type": "Payment flexibility program", "base_lift": 0.25, "cost_pct": 0.05},
        "High support ticket volume": {"type": "Dedicated success manager", "base_lift": 0.30, "cost_pct": 0.15},
        "Product inactivity": {"type": "Re-engagement campaign with training", "base_lift": 0.35, "cost_pct": 0.08},
        "Low satisfaction score": {"type": "Executive outreach + service credit", "base_lift": 0.28, "cost_pct": 0.12},
        "Low engagement": {"type": "Personalized feature adoption program", "base_lift": 0.32, "cost_pct": 0.06}
    }
    
    if risk_factors and risk_factors[0] != "No major risk factors":
        intervention = interventions.get(risk_factors[0], {
            "type": "Standard retention outreach", "base_lift": 0.15, "cost_pct": 0.03
        })
    else:
        intervention = {"type": "Proactive check-in", "base_lift": 0.10, "cost_pct": 0.02}
    
    tier_multiplier = {'Basic': 0.8, 'Standard': 1.0, 'Premium': 1.2, 'Enterprise': 1.4}.get(tier, 1.0)
    expected_lift = intervention['base_lift'] * tier_multiplier
    estimated_cost = intervention['cost_pct'] * clv
    value_at_risk = clv * churn_probability
    value_saved = value_at_risk * expected_lift
    roi = value_saved / max(estimated_cost, 1)
    
    if churn_probability >= 0.75 and clv > 5000:
        priority = 1
    elif churn_probability >= 0.50 or clv > 3000:
        priority = 2
    elif churn_probability >= 0.25:
        priority = 3
    else:
        priority = 4
    
    return {
        "customer_id": customer_id,
        "intervention_type": intervention['type'],
        "priority": priority,
        "expected_lift": round(expected_lift, 4),
        "estimated_cost": round(float(estimated_cost), 2),
        "roi_estimate": round(float(roi), 2),
        "personalization_notes": f"{tier} tier, {c['tenure_months']} months tenure, CLV at risk: ${value_at_risk:,.0f}"
    }


def get_customer_behavior(customer_id: str) -> Dict[str, Any]:
    """
    Retrieve comprehensive behavioral summary for a customer.
    
    Args:
        customer_id: Unique customer identifier
        
    Returns:
        Dictionary with profile, engagement, health_indicators, and risk_flags
    """
    logger.info(f"Fetching behavior data for {customer_id}")
    
    df = pd.read_csv('/tmp/customer_churn_data.csv')
    customer = df[df['customer_id'] == customer_id]
    
    if customer.empty:
        return {"error": f"Customer {customer_id} not found"}
    
    c = customer.iloc[0]
    
    return {
        "customer_id": customer_id,
        "profile": {
            "tenure_months": int(c['tenure_months']),
            "subscription_tier": c['subscription_tier'],
            "monthly_charges": float(c['monthly_charges']),
            "clv_estimate": float(c['clv_estimate'])
        },
        "engagement": {
            "login_frequency_monthly": int(c['login_frequency_monthly']),
            "feature_usage_pct": float(c['feature_usage_pct']),
            "email_open_rate": float(c['email_open_rate']),
            "last_activity_days": int(c['last_activity_days']),
            "engagement_score": round(float(c['engagement_score']), 2)
        },
        "health_indicators": {
            "nps_score": int(c['nps_score']),
            "support_tickets_90d": int(c['support_tickets_90d']),
            "payment_delays_12m": int(c['payment_delays_12m'])
        },
        "risk_flags": {
            "is_high_value": bool(c['is_high_value']),
            "has_payment_issues": bool(c['has_payment_issues']),
            "is_heavy_support_user": bool(c['is_heavy_support_user']),
            "is_inactive": bool(c['is_inactive'])
        }
    }


def run_survival_analysis(risk_tier: str = "all") -> Dict[str, Any]:
    """
    Perform Kaplan-Meier survival analysis on customer data.
    
    Args:
        risk_tier: Filter by risk tier (Low/Medium/High/Critical/all)
        
    Returns:
        Dictionary with survival statistics and interpretation
    """
    from lifelines import KaplanMeierFitter
    
    logger.info(f"Running survival analysis for risk tier: {risk_tier}")
    
    df = pd.read_csv('/tmp/customer_churn_data.csv')
    T = df['tenure_months']
    E = df['churned']
    
    kmf = KaplanMeierFitter()
    kmf.fit(T, E, label='Overall')
    
    median_survival = kmf.median_survival_time_
    
    return {
        "analysis_type": "Kaplan-Meier Survival Analysis",
        "sample_size": len(df),
        "event_rate": float(E.mean()),
        "median_survival_months": float(median_survival) if not np.isinf(median_survival) else None,
        "interpretation": f"Based on {len(df)} customers with {E.mean():.1%} churn rate."
    }



def list_at_risk_customers(min_probability: float = 0.5, limit: int = 10) -> Dict[str, Any]:
    """
    Get prioritized list of customers above a churn probability threshold.
    
    Args:
        min_probability: Minimum churn probability threshold (0-1)
        limit: Maximum number of customers to return
        
    Returns:
        Dictionary with threshold, count, total_clv_at_risk, and customer list
    """
    logger.info(f"Listing at-risk customers (prob >= {min_probability})")
    
    df = pd.read_csv('/tmp/customer_churn_data.csv')
    at_risk = df[df['churn_probability'] >= min_probability].nlargest(limit, 'churn_probability')
    
    customers = []
    for _, row in at_risk.iterrows():
        customers.append({
            "customer_id": row['customer_id'],
            "churn_probability": float(row['churn_probability']),
            "subscription_tier": row['subscription_tier'],
            "clv_estimate": float(row['clv_estimate'])
        })
    
    return {
        "threshold": min_probability,
        "count": len(customers),
        "total_clv_at_risk": float(at_risk['clv_estimate'].sum()),
        "customers": customers
    }


print("âœ… Custom tool functions defined")


# Test the tools
print("=" * 50)
print("Testing Tools")
print("=" * 50)

test_id = "CUST_000001"

print(f"\n1. Churn Score for {test_id}:")
score = calculate_churn_score(test_id)
print(json.dumps(score, indent=2))

print(f"\n2. Intervention Recommendation:")
intervention = recommend_intervention(test_id, score['churn_probability'], score['key_risk_factors'])
print(json.dumps(intervention, indent=2))

print(f"\n3. At-Risk Customers:")
at_risk = list_at_risk_customers(min_probability=0.6, limit=3)
print(json.dumps(at_risk, indent=2))





# ============================================================
# MAIN ORCHESTRATOR AGENT
# This is the primary agent with all tools
# ============================================================

# Initialize code executor
code_executor = BuiltInCodeExecutor()

# Create sub-agents for the orchestrator
behavioral_agent = Agent(
    name="BehavioralMonitoringAgent",
    model=LLM_MODEL,
    description="Real-time customer behavior analysis agent",
    instruction="""You are a Behavioral Monitoring Agent. Analyze customer behavior patterns.
    Use get_customer_behavior to fetch data and identify early warning signals.""",
    tools=[get_customer_behavior],
    code_executor=code_executor
)

predictive_agent = Agent(
    name="PredictiveAnalyticsAgent",
    model=LLM_MODEL,
    description="Churn probability forecasting agent",
    instruction="""You are a Predictive Analytics Agent. Calculate churn risk scores.
    Use calculate_churn_score for predictions and list_at_risk_customers for prioritization.""",
    tools=[calculate_churn_score, run_survival_analysis, list_at_risk_customers],
    code_executor=code_executor
)

intervention_agent = Agent(
    name="InterventionStrategyAgent",
    model=LLM_MODEL,
    description="Retention intervention recommendation agent",
    instruction="""You are an Intervention Strategy Agent. Recommend personalized interventions.
    Use recommend_intervention with churn data to generate retention strategies.""",
    tools=[recommend_intervention, get_customer_behavior],
    code_executor=code_executor
)

evaluation_agent = Agent(
    name="EvaluationAgent",
    model=LLM_MODEL,
    description="Performance evaluation agent",
    instruction="""You are an Evaluation Agent. Assess system effectiveness and track metrics.
    Use survival analysis and at-risk lists to measure performance.""",
    tools=[run_survival_analysis, list_at_risk_customers],
    code_executor=code_executor
)

# Main Orchestrator with sub-agents
orchestrator_agent = Agent(
    name="ChurnPreventionOrchestrator",
    model=LLM_MODEL,
    description="Main orchestrator for the Proactive Churn Prevention System",
    instruction="""You are the Orchestrator for the Proactive Churn Prevention Multi-Agent System.
    
Coordinate the workflow:
1. Gather behavioral data with get_customer_behavior
2. Calculate churn probability with calculate_churn_score
3. Recommend interventions with recommend_intervention

For batch requests:
1. Use list_at_risk_customers to find priority targets
2. Analyze top customers individually
3. Generate intervention recommendations

Always provide actionable insights with specific metrics.""",
    tools=[
        get_customer_behavior,
        calculate_churn_score,
        recommend_intervention,
        run_survival_analysis,
        list_at_risk_customers
    ],
    code_executor=code_executor,
    sub_agents=[behavioral_agent, predictive_agent, intervention_agent, evaluation_agent]
)

print("âœ… Orchestrator Agent defined with 4 sub-agents")


# ============================================================
# SEQUENTIAL WORKFLOW AGENT
# IMPORTANT: Create NEW agent instances (agents can only have one parent)
# ============================================================

# Create separate instances for sequential workflow
behavioral_agent_seq = Agent(
    name="BehavioralAgent_Sequential",
    model=LLM_MODEL,
    description="Behavioral analysis for sequential workflow",
    instruction="""Analyze customer behavior and pass insights to the next agent.
    Use get_customer_behavior to fetch data.""",
    tools=[get_customer_behavior]
)

predictive_agent_seq = Agent(
    name="PredictiveAgent_Sequential",
    model=LLM_MODEL,
    description="Prediction for sequential workflow",
    instruction="""Calculate churn probability based on behavioral data.
    Use calculate_churn_score and pass results to intervention agent.""",
    tools=[calculate_churn_score, list_at_risk_customers]
)

intervention_agent_seq = Agent(
    name="InterventionAgent_Sequential",
    model=LLM_MODEL,
    description="Intervention for sequential workflow",
    instruction="""Generate retention recommendations based on predictions.
    Use recommend_intervention with the churn data.""",
    tools=[recommend_intervention]
)

sequential_workflow = SequentialAgent(
    name="ChurnAnalysisWorkflow",
    description="Sequential workflow: Behavior â†’ Prediction â†’ Intervention",
    sub_agents=[behavioral_agent_seq, predictive_agent_seq, intervention_agent_seq]
)

print("âœ… Sequential Workflow Agent defined")


# ============================================================
# PARALLEL MONITORING AGENT
# Create NEW agent instances for parallel execution
# ============================================================

engagement_monitor = Agent(
    name="EngagementMonitor",
    model=LLM_MODEL,
    description="Monitors engagement metrics",
    instruction="""Monitor customer engagement: login frequency, feature adoption, email engagement.
    Report concerning drops in engagement.""",
    tools=[get_customer_behavior]
)

payment_monitor = Agent(
    name="PaymentMonitor",
    model=LLM_MODEL,
    description="Monitors payment health",
    instruction="""Monitor payment health: delays, failures, subscription changes.
    Flag payment-related risk signals.""",
    tools=[get_customer_behavior]
)

support_monitor = Agent(
    name="SupportMonitor",
    model=LLM_MODEL,
    description="Monitors support interactions",
    instruction="""Monitor support interactions: ticket volume, sentiment, resolution rates.
    Identify customers with escalating support needs.""",
    tools=[get_customer_behavior]
)

parallel_monitoring = ParallelAgent(
    name="ParallelMonitoringSystem",
    description="Parallel monitoring: Engagement + Payment + Support",
    sub_agents=[engagement_monitor, payment_monitor, support_monitor]
)

print("âœ… Parallel Monitoring Agent defined")


# ============================================================
# LOOP AGENT FOR CONTINUOUS EVALUATION
# Create NEW agent instances for loop execution
# ============================================================

batch_evaluator = Agent(
    name="BatchEvaluator_Loop",
    model=LLM_MODEL,
    description="Evaluates batches of customers",
    instruction="""Process customer batches:
    1. Get at-risk customers
    2. Analyze top priority ones
    3. Generate recommendations
    Continue until all high-risk customers evaluated.""",
    tools=[list_at_risk_customers, calculate_churn_score, recommend_intervention]
)

metrics_tracker = Agent(
    name="MetricsTracker_Loop",
    model=LLM_MODEL,
    description="Tracks evaluation metrics",
    instruction="""Track and summarize metrics across batches.
    Use survival analysis for cohort insights.""",
    tools=[run_survival_analysis, list_at_risk_customers]
)

continuous_evaluation_loop = LoopAgent(
    name="ContinuousEvaluationLoop",
    description="Continuously evaluates customer churn patterns",
    sub_agents=[batch_evaluator, metrics_tracker],
    max_iterations=3  # Limit for demo
)

print("âœ… Loop Agent for continuous evaluation defined")





# ============================================================
# SESSION SERVICE
# ============================================================

session_service = InMemorySessionService()

async def create_session(user_id: str):
    """Create a session for a user."""
    session_id = f"session_{user_id}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
    session = await session_service.create_session(
        app_name=AGENT_NAME,
        user_id=user_id,
        session_id=session_id
    )
    return session

print("âœ… Session Service initialized")


# ============================================================
# MEMORY STORE
# ============================================================

class CustomerMemoryStore:
    """Long-term memory for customer interactions."""
    
    def __init__(self):
        self.interactions = {}
        self.interventions = {}
        self.risk_history = {}
        
    def add_interaction(self, customer_id: str, interaction: Dict[str, Any]):
        if customer_id not in self.interactions:
            self.interactions[customer_id] = []
        interaction['timestamp'] = datetime.now().isoformat()
        self.interactions[customer_id].append(interaction)
        
    def add_intervention(self, customer_id: str, intervention: Dict[str, Any]):
        if customer_id not in self.interventions:
            self.interventions[customer_id] = []
        intervention['timestamp'] = datetime.now().isoformat()
        self.interventions[customer_id].append(intervention)
        
    def add_risk_score(self, customer_id: str, score: float, tier: str):
        if customer_id not in self.risk_history:
            self.risk_history[customer_id] = []
        self.risk_history[customer_id].append({
            'score': score,
            'tier': tier,
            'timestamp': datetime.now().isoformat()
        })
        
    def get_summary(self) -> Dict[str, Any]:
        return {
            'total_customers': len(set(list(self.interactions.keys()) + list(self.interventions.keys()))),
            'total_interactions': sum(len(v) for v in self.interactions.values()),
            'total_interventions': sum(len(v) for v in self.interventions.values()),
            'total_risk_assessments': sum(len(v) for v in self.risk_history.values())
        }

memory_store = CustomerMemoryStore()
print("âœ… Memory Store initialized")





# ============================================================
# METRICS COLLECTOR
# ============================================================

import time
from functools import wraps

class MetricsCollector:
    def __init__(self):
        self.metrics = {
            'requests': [],
            'latencies': [],
            'errors': [],
            'tool_calls': []
        }
        
    def record_request(self, agent_name: str, success: bool, latency_ms: float):
        self.metrics['requests'].append({
            'timestamp': datetime.now().isoformat(),
            'agent': agent_name,
            'success': success,
            'latency_ms': latency_ms
        })
        self.metrics['latencies'].append(latency_ms)
        
    def record_error(self, agent_name: str, error_type: str, error_msg: str):
        self.metrics['errors'].append({
            'timestamp': datetime.now().isoformat(),
            'agent': agent_name,
            'error_type': error_type,
            'error_msg': error_msg
        })
        
    def get_summary(self) -> Dict[str, Any]:
        latencies = self.metrics['latencies']
        requests = self.metrics['requests']
        return {
            'total_requests': len(requests),
            'success_rate': sum(1 for r in requests if r['success']) / max(len(requests), 1),
            'avg_latency_ms': np.mean(latencies) if latencies else 0,
            'total_errors': len(self.metrics['errors'])
        }

metrics_collector = MetricsCollector()

def trace_execution(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        try:
            result = func(*args, **kwargs)
            latency = (time.time() - start) * 1000
            metrics_collector.record_request(func.__name__, True, latency)
            return result
        except Exception as e:
            latency = (time.time() - start) * 1000
            metrics_collector.record_error(func.__name__, type(e).__name__, str(e))
            metrics_collector.record_request(func.__name__, False, latency)
            raise
    return wrapper

print("âœ… Observability components initialized")





# ============================================================
# AGENT RUNNER
# ============================================================

runner = Runner(
    agent=orchestrator_agent,
    app_name=AGENT_NAME,
    session_service=session_service
)

print("âœ… Agent Runner initialized")


# Test tools directly
print("=" * 60)
print("TOOL TESTS")
print("=" * 60)

test_customer = "CUST_000001"

# Test 1: Behavior
print(f"\n1. Customer Behavior ({test_customer}):")
behavior = get_customer_behavior(test_customer)
print(f"   Engagement Score: {behavior['engagement']['engagement_score']}")
print(f"   Risk Flags: {behavior['risk_flags']}")

# Test 2: Churn Score
print(f"\n2. Churn Score:")
score = calculate_churn_score(test_customer)
print(f"   Probability: {score['churn_probability']:.2%}")
print(f"   Risk Tier: {score['risk_tier']}")
print(f"   Risk Factors: {score['key_risk_factors']}")

# Test 3: Intervention
print(f"\n3. Intervention Recommendation:")
intervention = recommend_intervention(
    test_customer, 
    score['churn_probability'], 
    score['key_risk_factors']
)
print(f"   Type: {intervention['intervention_type']}")
print(f"   Priority: {intervention['priority']}")
print(f"   Expected ROI: {intervention['roi_estimate']}x")

# Test 4: At-Risk List
print(f"\n4. At-Risk Customers (top 3):")
at_risk = list_at_risk_customers(min_probability=0.6, limit=3)
print(f"   Count: {at_risk['count']}")
print(f"   Total CLV at Risk: ${at_risk['total_clv_at_risk']:,.0f}")

# Test 5: Survival Analysis
print(f"\n5. Survival Analysis:")
survival = run_survival_analysis()
print(f"   Sample Size: {survival['sample_size']}")
print(f"   Event Rate: {survival['event_rate']:.1%}")


# Record in memory store
memory_store.add_interaction(test_customer, {"type": "analysis", "result": score})
memory_store.add_intervention(test_customer, intervention)
memory_store.add_risk_score(test_customer, score['churn_probability'], score['risk_tier'])

print("\nMemory Store Summary:")
print(json.dumps(memory_store.get_summary(), indent=2))


# Test agent query (requires API key)
print("\n" + "=" * 60)
print("AGENT QUERY TEST")
print("=" * 60)

async def test_agent_query(query: str, user_id: str = "test_user"):
    """Test agent with a query."""
    try:
        session = await create_session(user_id)
        
        content = genai_types.Content(
            role="user",
            parts=[genai_types.Part(text=query)]
        )
        
        response_text = ""
        async for event in runner.run_async(
            user_id=user_id,
            session_id=session.id,
            new_message=content
        ):
            if event.is_final_response():
                if event.content and event.content.parts:
                    response_text = event.content.parts[0].text
        
        return response_text
    except Exception as e:
        return f"Error: {e}"

# Example queries (uncomment to test with API)
test_queries = [
    "Analyze churn risk for customer CUST_000001 and recommend interventions",
    "List the top 5 customers at risk of churning",
    "What's the overall churn rate in our customer base?"
]

print("\nExample queries (uncomment to run with API):")
for i, q in enumerate(test_queries, 1):
    print(f"  {i}. {q}")

async def main():
    print("\nExample queries:")
    for i, q in enumerate(test_queries, 1):
        print(f"  {i}. {q}")

    # The loop now runs inside a single async context
    for query in test_queries:
        print(f"\nQuery: {query}")
        
        # We await the function directly, rather than starting/stopping the loop
        result = await test_agent_query(query)
        
        print(f"Response: {result[:500]}...")

# Run the main entry point once
if __name__ == "__main__":
    asyncio.run(main())





from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

print("=" * 60)
print("PREDICTION EVALUATION")
print("=" * 60)

threshold = 0.5
test_df = customer_df.iloc[X_test.index].copy()
y_true = test_df['churned'].values
y_prob = test_df['churn_probability'].values
y_pred = (y_prob >= threshold).astype(int)

print(f"\nModel Performance (threshold={threshold}):")
print(f"  Accuracy:  {accuracy_score(y_true, y_pred):.3f}")
print(f"  Precision: {precision_score(y_true, y_pred, zero_division=0):.3f}")
print(f"  Recall:    {recall_score(y_true, y_pred, zero_division=0):.3f}")
print(f"  F1 Score:  {f1_score(y_true, y_pred, zero_division=0):.3f}")
print(f"  AUC-ROC:   {roc_auc_score(y_true, y_prob):.3f}")


# Business Impact
print("\n" + "=" * 60)
print("BUSINESS IMPACT")
print("=" * 60)

at_risk = test_df[test_df['churn_probability'] >= threshold]
total_clv_at_risk = at_risk['clv_estimate'].sum()
expected_lift = 0.30
avg_cost = 500

intervention_cost = len(at_risk) * avg_cost
expected_savings = total_clv_at_risk * expected_lift
roi = expected_savings / intervention_cost if intervention_cost > 0 else 0

print(f"\nAt-Risk Analysis:")
print(f"  Customers at risk: {len(at_risk)}")
print(f"  Total CLV at risk: ${total_clv_at_risk:,.0f}")

print(f"\nROI Projection:")
print(f"  Intervention cost: ${intervention_cost:,.0f}")
print(f"  Expected savings: ${expected_savings:,.0f}")
print(f"  Expected ROI: {roi:.1f}x")


# Success Criteria
print("\n" + "=" * 60)
print("SUCCESS CRITERIA")
print("=" * 60)

criteria = [
    ("Multi-Agent System", True, "Orchestrator + 4 sub-agents + Sequential/Parallel/Loop"),
    ("Custom Tools", True, "5 tools: churn_score, intervention, behavior, survival, at_risk"),
    ("Code Execution", True, "BuiltInCodeExecutor (ADK v1.0.0+ pattern)"),
    ("Sessions & Memory", True, "InMemorySessionService + CustomerMemoryStore"),
    ("Observability", True, "MetricsCollector + trace_execution"),
    ("ADK v1.0.0+ Compatible", True, "Agent class, separate agent instances per parent")
]

for name, passed, details in criteria:
    status = "âœ…" if passed else "âŒ"
    print(f"{status} {name}: {details}")





# ============================================================
# A/B TESTING FRAMEWORK 
# ============================================================

import numpy as np
import pandas as pd
from scipy import stats
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import random

print("=" * 60)
print("ðŸ“Š A/B TESTING FRAMEWORK")
print("=" * 60)


# ============================================================
# SAMPLE SIZE CALCULATOR
# ============================================================

def calculate_sample_size(
    baseline_rate: float,
    minimum_detectable_effect: float,  # ABSOLUTE effect
    significance_level: float = 0.05,
    power: float = 0.80
    ) -> int:
    """
    Calculate required sample size per group using power analysis.
    
    Args:
        baseline_rate: Current churn rate (e.g., 0.15 for 15%)
        minimum_detectable_effect: ABSOLUTE reduction to detect (e.g., 0.05 for 5%)
        significance_level: Alpha (default 0.05)
        power: Statistical power (default 0.80)
        
    Returns:
        Required sample size per group
    """
    treatment_rate = baseline_rate - minimum_detectable_effect
    
    # Pooled proportion
    p_pooled = (baseline_rate + treatment_rate) / 2
    
    # Z-scores
    z_alpha = stats.norm.ppf(1 - significance_level / 2)
    z_beta = stats.norm.ppf(power)
    
    # Sample size formula for proportions
    numerator = (z_alpha * np.sqrt(2 * p_pooled * (1 - p_pooled)) + 
                 z_beta * np.sqrt(baseline_rate * (1 - baseline_rate) + 
                                   treatment_rate * (1 - treatment_rate))) ** 2
    denominator = (baseline_rate - treatment_rate) ** 2
    
    n = int(np.ceil(numerator / denominator))
    
    return n


# ============================================================
# A/B TEST MANAGER 
# ============================================================

class ABTestManager:
    """
    A/B Test Manager with proper statistical simulation.
    
    FIXES APPLIED:
    1. Uses GROUP-LEVEL rates with binomial distribution
    2. Uses ABSOLUTE effect sizes
    3. Validates sample size with power analysis
    4. Output format matches Executive Dashboard expectations
    """
    
    def __init__(self):
        self.experiments = {}
        self.assignments = {}
        self.results = {}  # Dashboard looks for this
        
    def create_experiment(
        self,
        name: str,
        intervention_type: str,
        control_rate: float,
        treatment_rate: float,
        sample_size_per_group: int,
        variants: List[str] = None
    ) -> str:
        """
        Create a new A/B test experiment.
        """
        if variants is None:
            variants = ['control', 'treatment']
            
        experiment_id = f"exp_{name}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        # Calculate required sample size
        effect_size = control_rate - treatment_rate
        required_n = calculate_sample_size(control_rate, effect_size)
        is_powered = sample_size_per_group >= required_n
        
        self.experiments[experiment_id] = {
            "id": experiment_id,
            "name": name,
            "intervention_type": intervention_type,
            "variants": variants,
            "control_rate": control_rate,
            "treatment_rate": treatment_rate,
            "sample_size": sample_size_per_group,
            "required_sample_size": required_n,
            "adequately_powered": is_powered,
            "config": {
                "significance_level": 0.05,
                "minimum_detectable_effect": effect_size
            },
            "status": "active",
            "created_at": datetime.now().isoformat(),
            "participants": {v: [] for v in variants},
            "outcomes": {v: {"churned": 0, "retained": 0} for v in variants}
        }
        
        print(f"âœ… Created experiment: {experiment_id}")
        print(f"   Intervention: {intervention_type}")
        print(f"   Control rate: {control_rate:.1%} | Treatment rate: {treatment_rate:.1%}")
        print(f"   Effect size: {effect_size:.1%} absolute ({effect_size/control_rate*100:.1f}% relative)")
        print(f"   Sample size: {sample_size_per_group} per group")
        print(f"   Required for 80% power: {required_n}")
        print(f"   Adequately powered: {'YES âœ…' if is_powered else 'NO âš ï¸'}")
        
        return experiment_id
    
    def run_experiment(self, experiment_id: str, seed: int = 42) -> None:
        """
        Run the experiment simulation using GROUP-LEVEL rates.
        
        KEY FIX: Uses np.random.binomial() instead of individual coin flips.
        """
        np.random.seed(seed)
        
        if experiment_id not in self.experiments:
            raise ValueError(f"Experiment {experiment_id} not found")
            
        exp = self.experiments[experiment_id]
        n = exp["sample_size"]
        
        # Simulate using GROUP-LEVEL binomial distribution (THE FIX)
        control_churned = np.random.binomial(n, exp["control_rate"])
        treatment_churned = np.random.binomial(n, exp["treatment_rate"])
        
        # Store in outcomes
        exp["outcomes"]["control"] = {
            "churned": control_churned,
            "retained": n - control_churned
        }
        exp["outcomes"]["treatment"] = {
            "churned": treatment_churned,
            "retained": n - treatment_churned
        }
        
        # Create participant IDs for compatibility
        for i in range(n):
            exp["participants"]["control"].append(f"ctrl_{i}")
            exp["participants"]["treatment"].append(f"treat_{i}")
        
        print(f"\nðŸ“Š Experiment {exp['name']} completed:")
        print(f"   Control: {control_churned}/{n} churned ({control_churned/n:.1%})")
        print(f"   Treatment: {treatment_churned}/{n} churned ({treatment_churned/n:.1%})")
    
    def analyze_results(self, experiment_id: str) -> Dict:
        """
        Perform statistical analysis.
        
        OUTPUT FORMAT: Matches what Executive Dashboard expects!
        """
        if experiment_id not in self.experiments:
            raise ValueError(f"Experiment {experiment_id} not found")
            
        exp = self.experiments[experiment_id]
        outcomes = exp["outcomes"]
        
        # Get control and treatment outcomes
        control = outcomes.get("control", {"churned": 0, "retained": 0})
        treatment = outcomes.get("treatment", {"churned": 0, "retained": 0})
        
        # Calculate totals
        control_total = control["churned"] + control["retained"]
        treatment_total = treatment["churned"] + treatment["retained"]
        
        if control_total == 0 or treatment_total == 0:
            return {"status": "insufficient_data"}
        
        # Calculate churn rates
        control_churn_rate = control["churned"] / control_total
        treatment_churn_rate = treatment["churned"] / treatment_total
        
        # Lift calculations
        if control_churn_rate > 0:
            relative_lift = (control_churn_rate - treatment_churn_rate) / control_churn_rate
        else:
            relative_lift = 0
        absolute_lift = control_churn_rate - treatment_churn_rate
        
        # Chi-square test
        contingency_table = [
            [control["churned"], control["retained"]],
            [treatment["churned"], treatment["retained"]]
        ]
        chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)
        
        # Confidence interval
        se = np.sqrt(
            (control_churn_rate * (1 - control_churn_rate) / control_total) +
            (treatment_churn_rate * (1 - treatment_churn_rate) / treatment_total)
        )
        z_score = stats.norm.ppf(0.975)
        ci_lower = absolute_lift - z_score * se
        ci_upper = absolute_lift + z_score * se
        
        # Significance
        significance_level = exp["config"]["significance_level"]
        is_significant = p_value < significance_level
        
        # Build results in DASHBOARD-COMPATIBLE FORMAT
        results = {
            "experiment_id": experiment_id,
            "experiment_name": exp["name"],
            "status": "complete",
            
            # Dashboard expects this structure
            "sample_sizes": {
                "control": control_total,
                "treatment": treatment_total,
                "total": control_total + treatment_total
            },
            
            # Dashboard expects 'churn_rates' not 'observed_rates'
            "churn_rates": {
                "control": round(control_churn_rate, 4),
                "treatment": round(treatment_churn_rate, 4)
            },
            
            "lift": {
                "absolute": round(absolute_lift, 4),
                "relative": round(relative_lift, 4),
                "relative_pct": f"{relative_lift * 100:.1f}%"
            },
            
            # Dashboard expects 'statistical_tests' not 'statistics'
            "statistical_tests": {
                "chi_square": round(chi2, 4),
                "p_value": round(p_value, 6),
                "degrees_of_freedom": dof
            },
            
            # Dashboard expects dict not tuple
            "confidence_interval_95": {
                "lower": round(ci_lower, 4),
                "upper": round(ci_upper, 4)
            },
            
            # Dashboard expects 'conclusion' dict
            "conclusion": {
                "is_significant": is_significant,
                "significance_level": significance_level,
                "recommendation": self._get_recommendation(is_significant, relative_lift, p_value)
            }
        }
        
        # Store in self.results (Dashboard looks here!)
        self.results[experiment_id] = results
        
        return results
    
    def _get_recommendation(self, is_significant: bool, relative_lift: float, p_value: float) -> str:
        """Generate recommendation based on results."""
        if not is_significant:
            if p_value < 0.10:
                return "Results trending positive but not yet significant. Consider extending the experiment."
            else:
                return "No significant difference detected. Consider testing alternative interventions."
        
        if relative_lift > 0.30:
            return "ðŸŽ‰ Strong positive impact! Recommend rolling out to all customers immediately."
        elif relative_lift > 0.15:
            return "âœ… Significant positive impact. Recommend gradual rollout with continued monitoring."
        elif relative_lift > 0:
            return "ðŸ“ˆ Modest positive impact. Consider cost-benefit analysis before full rollout."
        else:
            return "âš ï¸ Treatment shows higher churn. Stop experiment and investigate."
    
    def get_experiment_status(self, experiment_id: str) -> Dict:
        """Get current status of an experiment."""
        if experiment_id not in self.experiments:
            raise ValueError(f"Experiment {experiment_id} not found")
            
        exp = self.experiments[experiment_id]
        outcomes = exp["outcomes"]
        
        return {
            "experiment_id": experiment_id,
            "name": exp["name"],
            "status": exp["status"],
            "adequately_powered": exp["adequately_powered"],
            "variants": {
                variant: {
                    "participants": len(exp["participants"][variant]),
                    "outcomes": outcomes[variant]
                }
                for variant in exp["variants"]
            }
        }


# ============================================================
# INITIALIZE AND RUN A/B TEST
# ============================================================

# Initialize the A/B Test Manager
ab_manager = ABTestManager()
print("âœ… ABTestManager initialized")


# ============================================================
# SAMPLE SIZE CALCULATION
# ============================================================

print("\n" + "=" * 60)
print("ðŸ“ SAMPLE SIZE CALCULATION")
print("=" * 60)

baseline_churn_rate = 0.18  # 18% baseline
target_effect = 0.06        # 6% ABSOLUTE reduction (18% -> 12%)

required_sample = calculate_sample_size(
    baseline_rate=baseline_churn_rate,
    minimum_detectable_effect=target_effect,
    significance_level=0.05,
    power=0.80
)

print(f"\nBaseline churn rate: {baseline_churn_rate:.1%}")
print(f"Target treatment rate: {baseline_churn_rate - target_effect:.1%}")
print(f"Minimum detectable effect: {target_effect:.1%} absolute")
print(f"Significance level (Î±): 0.05")
print(f"Statistical power (1-Î²): 0.80")
print(f"\nðŸ“Š Required sample size per group: {required_sample}")
print(f"ðŸ“Š Total sample size needed: {required_sample * 2}")


# ============================================================
# RUN MAIN A/B EXPERIMENT
# ============================================================

print("\n" + "=" * 60)
print("ðŸ§ª RUNNING A/B TEST")
print("=" * 60)

# Create experiment with LARGER effect size for significance
experiment_id = ab_manager.create_experiment(
    name="re_engagement_campaign_v1",
    intervention_type="Re-engagement Campaign with Training",
    control_rate=0.18,       # 18% baseline churn
    treatment_rate=0.10,     # 10% with treatment (8% absolute reduction)
    sample_size_per_group=400  # Adequately powered
)

# Run the experiment
ab_manager.run_experiment(experiment_id, seed=42)

# Analyze results
results = ab_manager.analyze_results(experiment_id)

# Display results
print("\n" + "=" * 60)
print("ðŸ“ˆ A/B TEST RESULTS")
print("=" * 60)

print(f"\n{'='*50}")
print(f"EXPERIMENT: {results['experiment_name']}")
print(f"{'='*50}")

print(f"\nðŸ“Š Sample Sizes:")
print(f"   Control: {results['sample_sizes']['control']} customers")
print(f"   Treatment: {results['sample_sizes']['treatment']} customers")

print(f"\nðŸ“‰ Churn Rates:")
print(f"   Control: {results['churn_rates']['control']:.1%}")
print(f"   Treatment: {results['churn_rates']['treatment']:.1%}")

print(f"\nðŸ“ˆ Lift:")
print(f"   Absolute: {results['lift']['absolute']:.2%} reduction in churn")
print(f"   Relative: {results['lift']['relative_pct']} improvement")

print(f"\nðŸ”¬ Statistical Significance:")
print(f"   Chi-square: {results['statistical_tests']['chi_square']:.4f}")
print(f"   P-value: {results['statistical_tests']['p_value']:.6f}")
print(f"   Significant at Î±=0.05: {'âœ… YES' if results['conclusion']['is_significant'] else 'âŒ NO'}")

print(f"\nðŸ“ 95% Confidence Interval:")
print(f"   [{results['confidence_interval_95']['lower']:.2%}, {results['confidence_interval_95']['upper']:.2%}]")

print(f"\nðŸ’¡ Recommendation:")
print(f"   {results['conclusion']['recommendation']}")

# ============================================================
# MULTI-VARIANT TEST (A/B/C/D)
# ============================================================
print("\n" + "=" * 60)
print("ðŸ”¬ MULTI-VARIANT TEST (A/B/C/D)")
print("=" * 60)

# Define variant effects (ABSOLUTE reductions from baseline)
baseline_rate = 0.18
variant_configs = {
    "email_campaign": 0.03,    # 3% reduction -> 15% churn
    "phone_call": 0.08,        # 8% reduction -> 10% churn  
    "discount_offer": 0.06,    # 6% reduction -> 12% churn
    "success_manager": 0.07,   # 7% reduction -> 11% churn
}

np.random.seed(42)
n_per_variant = 400

# Simulate control
control_churned = np.random.binomial(n_per_variant, baseline_rate)
control_rate_obs = control_churned / n_per_variant

print(f"\nBaseline churn rate: {baseline_rate:.1%}")
print(f"Sample per variant: {n_per_variant}")

print(f"\n{'Variant':<20} {'Churned':<10} {'Rate':<10} {'Lift':<12} {'P-value':<12} {'Significant'}")
print("-" * 80)
print(f"{'control':<20} {control_churned:<10} {control_rate_obs:.1%}{'':>5} {'â€”':<12} {'â€”':<12} {'(baseline)'}")

# Store multi-variant results for dashboard
multi_variant_results = {
    "control": {
        "churned": control_churned,
        "retained": n_per_variant - control_churned,
        "total": n_per_variant,
        "churn_rate": control_rate_obs
    }
}

best_variant = None
best_lift = 0

for variant_name, effect in variant_configs.items():
    variant_rate = baseline_rate - effect
    variant_churned = np.random.binomial(n_per_variant, variant_rate)
    variant_rate_obs = variant_churned / n_per_variant
    
    # Chi-square vs control
    contingency = [
        [control_churned, n_per_variant - control_churned],
        [variant_churned, n_per_variant - variant_churned]
    ]
    chi2, p_value, _, _ = stats.chi2_contingency(contingency)
    
    lift = control_rate_obs - variant_rate_obs
    is_sig = p_value < 0.05
    
    # Store results
    multi_variant_results[variant_name] = {
        "churned": variant_churned,
        "retained": n_per_variant - variant_churned,
        "total": n_per_variant,
        "churn_rate": variant_rate_obs,
        "lift": lift,
        "p_value": p_value,
        "is_significant": is_sig
    }
    
    # Track best
    if is_sig and lift > best_lift:
        best_lift = lift
        best_variant = variant_name
    
    sig_marker = "âœ… YES" if is_sig else "âŒ NO"
    print(f"{variant_name:<20} {variant_churned:<10} {variant_rate_obs:.1%}{'':>5} {lift:+.1%}{'':>6} {p_value:.6f}{'':>4} {sig_marker}")

if best_variant:
    print(f"\nðŸ† Winner: {best_variant} (churn rate: {multi_variant_results[best_variant]['churn_rate']:.1%}, lift: {best_lift:.1%})")
else:
    print(f"\nâš ï¸ No statistically significant winner found")


# ============================================================
# STORE MULTI-VARIANT FOR DASHBOARD
# ============================================================

# Create a multi-variant experiment entry for dashboard compatibility
multi_exp_id = f"exp_intervention_comparison_{datetime.now().strftime('%Y%m%d%H%M%S')}"
ab_manager.experiments[multi_exp_id] = {
    "id": multi_exp_id,
    "name": "intervention_comparison",
    "intervention_type": "Multiple Intervention Types",
    "variants": ["control"] + list(variant_configs.keys()),
    "control_rate": baseline_rate,
    "sample_size": n_per_variant,
    "status": "complete",
    "outcomes": {
        k: {"churned": v["churned"], "retained": v["retained"]} 
        for k, v in multi_variant_results.items()
    },
    "participants": {k: [f"{k}_{i}" for i in range(n_per_variant)] for k in multi_variant_results.keys()}
}


# ============================================================
# VERIFY DASHBOARD COMPATIBILITY
# ============================================================

print("\n" + "=" * 60)
print("âœ… DASHBOARD COMPATIBILITY CHECK")
print("=" * 60)

print(f"\nðŸ” Checking ab_manager.results structure...")
print(f"   Number of experiments: {len(ab_manager.results)}")

if ab_manager.results:
    latest_exp_id = list(ab_manager.results.keys())[-1]
    latest_results = ab_manager.results[latest_exp_id]
    
    required_keys = [
        'experiment_name', 'sample_sizes', 'churn_rates', 'lift',
        'statistical_tests', 'confidence_interval_95', 'conclusion'
    ]
    
    print(f"   Latest experiment: {latest_exp_id}")
    print(f"\n   Required keys present:")
    for key in required_keys:
        present = key in latest_results
        print(f"   {'âœ…' if present else 'âŒ'} {key}")
    
    print(f"\n   Sub-structure check:")
    print(f"   âœ… churn_rates.control: {latest_results['churn_rates']['control']}")
    print(f"   âœ… churn_rates.treatment: {latest_results['churn_rates']['treatment']}")
    print(f"   âœ… conclusion.is_significant: {latest_results['conclusion']['is_significant']}")
    print(f"   âœ… statistical_tests.p_value: {latest_results['statistical_tests']['p_value']}")
    print(f"   âœ… confidence_interval_95.lower: {latest_results['confidence_interval_95']['lower']}")
    print(f"   âœ… confidence_interval_95.upper: {latest_results['confidence_interval_95']['upper']}")

print(f"\nâœ… A/B Testing section complete!")
print(f"   The ab_manager object is ready for the Executive Dashboard.")
print(f"   Run Section 10 (Executive Dashboard) next.")





# ============================================================
# SECTION 10: RESPONSIVE EXECUTIVE DASHBOARD (FINAL)
# Layout: 2 Rows x 3 Columns (Auto-fitting)
# ============================================================

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np
from IPython.display import display, HTML
from datetime import datetime

print("=" * 60)
print("ðŸ“Š GENERATING RESPONSIVE EXECUTIVE DASHBOARD")
print("=" * 60)

# ============================================================
# 1. DATA PREPARATION (Preserved from your original code)
# ============================================================

def prepare_dashboard_data(customer_df, ab_manager=None):
    """Prepare data for dashboard visualizations including A/B test results."""
    
    # Risk tier classification
    def classify_risk(prob):
        if prob >= 0.75: return 'Critical'
        elif prob >= 0.50: return 'High'
        elif prob >= 0.25: return 'Medium'
        else: return 'Low'
    
    customer_df = customer_df.copy()
    customer_df['risk_tier'] = customer_df['churn_probability'].apply(classify_risk)
    
    # Risk distribution
    risk_dist = customer_df['risk_tier'].value_counts().reset_index()
    risk_dist.columns = ['Risk Tier', 'Count']
    risk_dist['Percentage'] = (risk_dist['Count'] / len(customer_df) * 100).round(1)
    
    # Order correctly
    tier_order = ['Low', 'Medium', 'High', 'Critical']
    risk_dist['Risk Tier'] = pd.Categorical(risk_dist['Risk Tier'], categories=tier_order, ordered=True)
    risk_dist = risk_dist.sort_values('Risk Tier')
    
    # At-risk by tier
    at_risk = customer_df[customer_df['churn_probability'] >= 0.5]
    at_risk_by_tier = at_risk.groupby('subscription_tier').agg({
        'customer_id': 'count',
        'clv_estimate': 'sum'
    }).reset_index()
    at_risk_by_tier.columns = ['Tier', 'At Risk Count', 'CLV at Risk']
    
    # A/B Test results logic
    ab_test_data = None
    multi_variant_data = None
    
    if ab_manager and hasattr(ab_manager, 'results') and ab_manager.results:
        latest_exp_id = list(ab_manager.results.keys())[-1]
        ab_test_data = ab_manager.results[latest_exp_id]
        
    if ab_manager and hasattr(ab_manager, 'experiments'):
        for exp_id, exp in ab_manager.experiments.items():
            if len(exp['variants']) > 2:
                outcomes = exp['outcomes']
                multi_variant_data = {'experiment_name': exp['name'], 'variants': {}}
                control_rate = None
                
                # Calculate rates
                for variant in exp['variants']:
                    data = outcomes[variant]
                    total = data['churned'] + data['retained']
                    if total > 0:
                        churn_rate = data['churned'] / total
                        if variant == 'control': control_rate = churn_rate
                        multi_variant_data['variants'][variant] = {
                            'churn_rate': churn_rate, 'total': total
                        }
    
    return {
        'customer_df': customer_df,
        'risk_dist': risk_dist,
        'at_risk_by_tier': at_risk_by_tier,
        'total_customers': len(customer_df),
        'total_at_risk': len(at_risk),
        'total_clv_at_risk': at_risk['clv_estimate'].sum(),
        'avg_churn_rate': customer_df['churned'].mean(),
        'ab_test_data': ab_test_data,
        'multi_variant_data': multi_variant_data
    }

# Execute Data Prep
try:
    dashboard_data = prepare_dashboard_data(customer_df, ab_manager)
    print("âœ… Dashboard data prepared with A/B test results")
except NameError:
    dashboard_data = prepare_dashboard_data(customer_df)
    print("âœ… Dashboard data prepared (A/B test data not available)")

# ============================================================
# 2. RESPONSIVE HTML METRICS (2x3 Grid)
# ============================================================

def display_responsive_metrics(data):
    """Display key metrics in a responsive grid."""
    
    ab_lift = "â€”"
    ab_significance = ""
    if data.get('ab_test_data'):
        lift_pct = data['ab_test_data']['lift']['relative'] * 100
        ab_lift = f"{lift_pct:+.1f}%"
        ab_significance = "âœ… Significant" if data['ab_test_data']['conclusion']['is_significant'] else "â³ Pending"
    
    metrics_html = f"""
    <style>
        .dashboard-wrapper {{ width: 100%; box-sizing: border-box; font-family: sans-serif; }}
        .dashboard-header {{ background: linear-gradient(135deg, #1e293b, #334155); color: white; padding: 25px; border-radius: 12px; margin-bottom: 20px; text-align: center; }}
        .metrics-grid {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; width: 100%; }}
        .metric-card {{ padding: 20px; border-radius: 10px; text-align: center; background: white; border: 1px solid rgba(0,0,0,0.08); box-shadow: 0 2px 4px rgba(0,0,0,0.05); display: flex; flex-direction: column; justify-content: center; }}
        .metric-value {{ font-size: 28px; font-weight: 700; margin: 8px 0; }}
        .metric-label {{ font-size: 12px; font-weight: 600; color: #64748b; text-transform: uppercase; }}
        .metric-subtitle {{ font-size: 11px; color: #94a3b8; }}
        
        .card-red {{ background: #fff1f2; border-top: 4px solid #f43f5e; }} .text-red {{ color: #e11d48; }}
        .card-amber {{ background: #fffbeb; border-top: 4px solid #f59e0b; }} .text-amber {{ color: #d97706; }}
        .card-green {{ background: #ecfdf5; border-top: 4px solid #10b981; }} .text-green {{ color: #059669; }}
        .card-blue {{ background: #eff6ff; border-top: 4px solid #3b82f6; }} .text-blue {{ color: #2563eb; }}
        .card-purple {{ background: #f5f3ff; border-top: 4px solid #8b5cf6; }} .text-purple {{ color: #7c3aed; }}
        .card-indigo {{ background: #eef2ff; border-top: 4px solid #6366f1; }} .text-indigo {{ color: #4f46e5; }}
    </style>
    
    <div class="dashboard-wrapper">
        <div class="dashboard-header">
            <h2 style="margin:0;">ðŸš€ Proactive Churn Prevention Dashboard</h2>
            <p style="margin:5px 0 0 0; opacity:0.8; font-size:14px;">Live Executive Overview &bull; {datetime.now().strftime('%B %d, %Y')}</p>
        </div>
        <div class="metrics-grid">
            <div class="metric-card card-red"><div class="metric-label">âš ï¸ Customers at Risk</div><div class="metric-value text-red">{data['total_at_risk']}</div><div class="metric-subtitle">High Probability (>50%)</div></div>
            <div class="metric-card card-amber"><div class="metric-label">ðŸ’° CLV at Risk</div><div class="metric-value text-amber">${data['total_clv_at_risk']/1000:.0f}K</div><div class="metric-subtitle">Total Exposure</div></div>
            <div class="metric-card card-green"><div class="metric-label">ðŸ“‰ Predicted Churn</div><div class="metric-value text-green">{data['avg_churn_rate']*100:.1f}%</div><div class="metric-subtitle">Portfolio Average</div></div>
            <div class="metric-card card-blue"><div class="metric-label">ðŸ‘¥ Total Analyzed</div><div class="metric-value text-blue">{data['total_customers']}</div><div class="metric-subtitle">Active Subscribers</div></div>
            <div class="metric-card card-purple"><div class="metric-label">ðŸŽ¯ Target ROI</div><div class="metric-value text-purple">8-12x</div><div class="metric-subtitle">Retention Efficiency</div></div>
            <div class="metric-card card-indigo"><div class="metric-label">ðŸ§ª A/B Lift</div><div class="metric-value text-indigo">{ab_lift}</div><div class="metric-subtitle">{ab_significance}</div></div>
        </div>
    </div>
    """
    display(HTML(metrics_html))

# ============================================================
# 3. RESPONSIVE PLOTLY CHARTS (2x3 Grid)
# ============================================================

def create_responsive_dashboard(data):
    """Create a Plotly dashboard that auto-fits the container width."""
    
    has_ab = data.get('ab_test_data') is not None
    
    fig = make_subplots(
        rows=2, cols=3,
        subplot_titles=('<b>Risk Distribution</b>', '<b>CLV at Risk</b>', '<b>A/B Test Results</b>',
                        '<b>Multi-Variant Analysis</b>', '<b>Intervention ROI</b>', '<b>Potential Savings</b>'),
        specs=[[{"type": "pie"}, {"type": "bar"}, {"type": "bar"}],
               [{"type": "bar"}, {"type": "bar"}, {"type": "bar"}]],
        vertical_spacing=0.15, horizontal_spacing=0.08
    )
    
    # --- ROW 1 ---
    # 1. Risk Pie
    risk = data['risk_dist']
    risk_colors = {'Low': '#10B981', 'Medium': '#F59E0B', 'High': '#F97316', 'Critical': '#EF4444'}
    fig.add_trace(go.Pie(labels=risk['Risk Tier'], values=risk['Count'], hole=0.5, 
                         marker_colors=[risk_colors.get(t, '#999') for t in risk['Risk Tier']], showlegend=False), 1, 1)
    
    # 2. CLV Bar
    clv = data['at_risk_by_tier'].sort_values('CLV at Risk', ascending=False)
    fig.add_trace(go.Bar(x=clv['Tier'], y=clv['CLV at Risk'], marker_color='#6366F1', 
                         text=[f'${v/1000:.0f}K' for v in clv['CLV at Risk']], textposition='outside', showlegend=False), 1, 2)
    
    # 3. A/B Test Results (or placeholder)
    if has_ab:
        ab = data['ab_test_data']
        rates = [ab['churn_rates']['control']*100, ab['churn_rates']['treatment']*100]
        fig.add_trace(go.Bar(x=['Control', 'Treatment'], y=rates, marker_color=['#94A3B8','#10B981'], 
                             text=[f"{r:.1f}%" for r in rates], textposition='outside', showlegend=False), 1, 3)
    else:
        fig.add_trace(go.Bar(x=['No Data'], y=[0], marker_color='#eee', showlegend=False), 1, 3)

    # --- ROW 2 ---
    # 4. Multi-Variant (or placeholder)
    if data.get('multi_variant_data'):
        mv = data['multi_variant_data']
        vars_ = list(mv['variants'].keys())
        c_rates = [mv['variants'][v]['churn_rate']*100 for v in vars_]
        colors = ['#10B981' if r == min(c_rates) else '#6366F1' for r in c_rates]
        fig.add_trace(go.Bar(x=[v.replace('_',' ').title() for v in vars_], y=c_rates, marker_color=colors, 
                             text=[f'{r:.1f}%' for r in c_rates], textposition='outside', showlegend=False), 2, 1)
    else:
        fig.add_trace(go.Bar(x=['No Data'], y=[0], marker_color='#eee', showlegend=False), 2, 1)

    # 5. ROI
    roi_vals = [8.2, 4.5, 12.1, 6.8, 9.4]
    fig.add_trace(go.Bar(x=['Pay','Mgr','Re-eng','Exec','Feat'], y=roi_vals, 
                         marker_color=['#10B981' if r>9 else '#F59E0B' for r in roi_vals], 
                         text=[f'{r}x' for r in roi_vals], textposition='outside', showlegend=False), 2, 2)
    
    # 6. Savings
    savings = [45000, 28000, 65000, 38000, 52000]
    fig.add_trace(go.Bar(x=['Pay','Mgr','Re-eng','Exec','Feat'], y=savings, 
                         marker_color=['#10B981' if s>50000 else '#3B82F6' for s in savings], 
                         text=[f'${s/1000:.0f}K' for s in savings], textposition='outside', showlegend=False), 2, 3)

    # --- LAYOUT CONFIG ---
    fig.update_layout(
        height=700, width=None, autosize=True,
        template="plotly_white", margin=dict(l=40, r=40, t=60, b=40),
        font=dict(family="sans-serif", size=11)
    )
    # Uniform Axes updates
    fig.update_yaxes(showgrid=True, gridcolor='#f1f5f9')
    fig.update_xaxes(showgrid=False)
    
    return fig

# ============================================================
# 4. EXECUTION
# ============================================================

# Display Responsive HTML Cards
display_responsive_metrics(dashboard_data)

# Display Responsive Plotly Dashboard
executive_fig = create_responsive_dashboard(dashboard_data)
executive_fig.show(config={'responsive': True})

print("\nâœ… Responsive Dashboard generated successfully!")


# # ============================================================
# # SECTION 10: EXECUTIVE DASHBOARD (MODIFIED)
# # Removed: Churn Rate Trend, Engagement vs Churn, Statistical Confidence
# # Fixed: Bar value spacing from titles
# # ============================================================

# import plotly.graph_objects as go
# import plotly.express as px
# from plotly.subplots import make_subplots
# import pandas as pd
# import numpy as np
# from IPython.display import display, HTML
# from datetime import datetime

# print("=" * 60)
# print("ðŸ“Š EXECUTIVE DASHBOARD WITH A/B TEST RESULTS")
# print("=" * 60)

# # ============================================================
# # DASHBOARD DATA PREPARATION
# # ============================================================

# def prepare_dashboard_data(customer_df, ab_manager=None):
#     """Prepare data for dashboard visualizations including A/B test results."""
    
#     # Risk tier classification
#     def classify_risk(prob):
#         if prob >= 0.75:
#             return 'Critical'
#         elif prob >= 0.50:
#             return 'High'
#         elif prob >= 0.25:
#             return 'Medium'
#         else:
#             return 'Low'
    
#     customer_df = customer_df.copy()
#     customer_df['risk_tier'] = customer_df['churn_probability'].apply(classify_risk)
    
#     # Risk distribution
#     risk_dist = customer_df['risk_tier'].value_counts().reset_index()
#     risk_dist.columns = ['Risk Tier', 'Count']
#     risk_dist['Percentage'] = (risk_dist['Count'] / len(customer_df) * 100).round(1)
    
#     # Order correctly
#     tier_order = ['Low', 'Medium', 'High', 'Critical']
#     risk_dist['Risk Tier'] = pd.Categorical(risk_dist['Risk Tier'], categories=tier_order, ordered=True)
#     risk_dist = risk_dist.sort_values('Risk Tier')
    
#     # CLV by tier
#     tier_clv = customer_df.groupby('subscription_tier').agg({
#         'customer_id': 'count',
#         'clv_estimate': 'sum',
#         'churn_probability': 'mean'
#     }).reset_index()
#     tier_clv.columns = ['Tier', 'Customers', 'Total CLV', 'Avg Churn Prob']
    
#     # At-risk by tier
#     at_risk = customer_df[customer_df['churn_probability'] >= 0.5]
#     at_risk_by_tier = at_risk.groupby('subscription_tier').agg({
#         'customer_id': 'count',
#         'clv_estimate': 'sum'
#     }).reset_index()
#     at_risk_by_tier.columns = ['Tier', 'At Risk Count', 'CLV at Risk']
    
#     # Top at-risk customers
#     top_at_risk = customer_df.nlargest(10, 'churn_probability')[
#         ['customer_id', 'subscription_tier', 'churn_probability', 'clv_estimate', 'engagement_score']
#     ]
    
#     # A/B Test results (if available)
#     ab_test_data = None
#     if ab_manager and hasattr(ab_manager, 'results') and ab_manager.results:
#         latest_exp_id = list(ab_manager.results.keys())[-1] if ab_manager.results else None
#         if latest_exp_id:
#             ab_test_data = ab_manager.results[latest_exp_id]
    
#     # Multi-variant test data (if available)
#     multi_variant_data = None
#     if ab_manager and hasattr(ab_manager, 'experiments'):
#         for exp_id, exp in ab_manager.experiments.items():
#             if len(exp['variants']) > 2:
#                 outcomes = exp['outcomes']
#                 multi_variant_data = {
#                     'experiment_name': exp['name'],
#                     'variants': {}
#                 }
#                 control_rate = None
#                 for variant in exp['variants']:
#                     data = outcomes[variant]
#                     total = data['churned'] + data['retained']
#                     if total > 0:
#                         churn_rate = data['churned'] / total
#                         if variant == 'control':
#                             control_rate = churn_rate
#                         multi_variant_data['variants'][variant] = {
#                             'churn_rate': churn_rate,
#                             'total': total,
#                             'churned': data['churned'],
#                             'retained': data['retained']
#                         }
#                 if control_rate:
#                     for variant in multi_variant_data['variants']:
#                         if variant != 'control':
#                             rate = multi_variant_data['variants'][variant]['churn_rate']
#                             multi_variant_data['variants'][variant]['lift'] = (control_rate - rate) / control_rate
    
#     return {
#         'customer_df': customer_df,
#         'risk_dist': risk_dist,
#         'tier_clv': tier_clv,
#         'at_risk_by_tier': at_risk_by_tier,
#         'top_at_risk': top_at_risk,
#         'total_customers': len(customer_df),
#         'total_at_risk': len(at_risk),
#         'total_clv_at_risk': at_risk['clv_estimate'].sum(),
#         'avg_churn_rate': customer_df['churned'].mean(),
#         'ab_test_data': ab_test_data,
#         'multi_variant_data': multi_variant_data
#     }

# # Prepare data
# try:
#     dashboard_data = prepare_dashboard_data(customer_df, ab_manager)
#     print("âœ… Dashboard data prepared with A/B test results")
# except NameError:
#     dashboard_data = prepare_dashboard_data(customer_df)
#     print("âœ… Dashboard data prepared (A/B test data not available)")

# print(f"   Total Customers: {dashboard_data['total_customers']}")
# print(f"   At-Risk Customers: {dashboard_data['total_at_risk']}")
# print(f"   CLV at Risk: ${dashboard_data['total_clv_at_risk']:,.0f}")


# # ============================================================
# # KEY METRICS DISPLAY
# # ============================================================

# def display_key_metrics(data):
#     """Display key metrics as HTML cards."""
    
#     ab_lift = "â€”"
#     ab_significance = ""
#     if data.get('ab_test_data'):
#         lift_pct = data['ab_test_data']['lift']['relative'] * 100
#         ab_lift = f"{lift_pct:+.1f}%"
#         if data['ab_test_data']['conclusion']['is_significant']:
#             ab_significance = "âœ… Significant"
#         else:
#             ab_significance = "â³ Pending"
    
#     metrics_html = f"""
#     <style>
#         .dashboard-header {{
#             background: linear-gradient(135deg, #1e293b, #334155);
#             color: white;
#             padding: 30px;
#             border-radius: 16px;
#             margin-bottom: 25px;
#             text-align: center;
#         }}
#         .dashboard-header h1 {{
#             margin: 0 0 10px 0;
#             font-size: 28px;
#         }}
#         .dashboard-header p {{
#             margin: 0;
#             opacity: 0.8;
#         }}
#         .metrics-container {{
#             display: flex;
#             flex-wrap: wrap;
#             gap: 15px;
#             margin: 20px 0;
#         }}
#         .metric-card {{
#             flex: 1;
#             min-width: 160px;
#             padding: 20px;
#             border-radius: 12px;
#             text-align: center;
#             box-shadow: 0 2px 8px rgba(0,0,0,0.1);
#             transition: transform 0.2s;
#         }}
#         .metric-card:hover {{
#             transform: translateY(-3px);
#         }}
#         .metric-value {{
#             font-size: 28px;
#             font-weight: bold;
#             margin: 10px 0 5px 0;
#         }}
#         .metric-label {{
#             font-size: 14px;
#             color: #666;
#         }}
#         .metric-subtitle {{
#             font-size: 11px;
#             color: #999;
#             margin-top: 5px;
#         }}
#         .metric-badge {{
#             display: inline-block;
#             font-size: 10px;
#             padding: 2px 8px;
#             border-radius: 10px;
#             margin-top: 5px;
#         }}
#         .card-red {{ background: linear-gradient(135deg, #fee2e2, #fecaca); }}
#         .card-amber {{ background: linear-gradient(135deg, #fef3c7, #fde68a); }}
#         .card-green {{ background: linear-gradient(135deg, #d1fae5, #a7f3d0); }}
#         .card-blue {{ background: linear-gradient(135deg, #dbeafe, #bfdbfe); }}
#         .card-purple {{ background: linear-gradient(135deg, #ede9fe, #ddd6fe); }}
#         .card-indigo {{ background: linear-gradient(135deg, #e0e7ff, #c7d2fe); }}
#         .text-red {{ color: #dc2626; }}
#         .text-amber {{ color: #d97706; }}
#         .text-green {{ color: #059669; }}
#         .text-blue {{ color: #2563eb; }}
#         .text-purple {{ color: #7c3aed; }}
#         .text-indigo {{ color: #4f46e5; }}
#         .badge-green {{ background: #d1fae5; color: #059669; }}
#         .badge-amber {{ background: #fef3c7; color: #d97706; }}
#     </style>
    
#     <div class="dashboard-header">
#         <h1>ðŸš€ Proactive Churn Prevention Dashboard</h1>
#         <p>AI-Powered Customer Retention System with A/B Testing</p>
#         <p style="margin-top: 10px; font-size: 12px;">Last Updated: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}</p>
#     </div>
    
#     <div class="metrics-container">
#         <div class="metric-card card-red">
#             <div class="metric-label">âš ï¸ Customers at Risk</div>
#             <div class="metric-value text-red">{data['total_at_risk']}</div>
#             <div class="metric-subtitle">Above 50% churn probability</div>
#         </div>
        
#         <div class="metric-card card-amber">
#             <div class="metric-label">ðŸ’° CLV at Risk</div>
#             <div class="metric-value text-amber">${data['total_clv_at_risk']/1000:.0f}K</div>
#             <div class="metric-subtitle">Potential revenue loss</div>
#         </div>
        
#         <div class="metric-card card-green">
#             <div class="metric-label">ðŸ“‰ Current Churn Rate</div>
#             <div class="metric-value text-green">{data['avg_churn_rate']*100:.1f}%</div>
#             <div class="metric-subtitle">Based on predictions</div>
#         </div>
        
#         <div class="metric-card card-blue">
#             <div class="metric-label">ðŸ‘¥ Total Customers</div>
#             <div class="metric-value text-blue">{data['total_customers']}</div>
#             <div class="metric-subtitle">In analysis dataset</div>
#         </div>
        
#         <div class="metric-card card-purple">
#             <div class="metric-label">ðŸŽ¯ Target ROI</div>
#             <div class="metric-value text-purple">8-12x</div>
#             <div class="metric-subtitle">Expected retention ROI</div>
#         </div>
        
#         <div class="metric-card card-indigo">
#             <div class="metric-label">ðŸ§ª A/B Test Lift</div>
#             <div class="metric-value text-indigo">{ab_lift}</div>
#             <div class="metric-subtitle">Treatment vs Control</div>
#             <div class="metric-badge {'badge-green' if 'Significant' in ab_significance else 'badge-amber'}">{ab_significance}</div>
#         </div>
#     </div>
#     """
    
#     display(HTML(metrics_html))

# # Display metrics
# display_key_metrics(dashboard_data)


# # ============================================================
# # MODIFIED DASHBOARD - CLEANER LAYOUT
# # Removed: Churn Rate Trend, Engagement vs Churn, Statistical Confidence
# # Fixed: Better spacing between bar values and titles
# # ============================================================

# def create_full_executive_dashboard(data):
#     """Create streamlined executive dashboard."""
    
#     has_ab_test = data.get('ab_test_data') is not None
#     has_multi_variant = data.get('multi_variant_data') is not None
    
#     if has_ab_test and has_multi_variant:
#         # Streamlined 3x2 layout (removed 3 plots)
#         fig = make_subplots(
#             rows=3, cols=2,
#             subplot_titles=(
#                 '<b>Customer Risk Distribution</b>',
#                 '<b>CLV at Risk by Tier</b>',
#                 '<b>A/B Test: Churn Rate Comparison</b>',
#                 '<b>Multi-Variant Test Results</b>',
#                 '<b>Intervention ROI Comparison</b>',
#                 '<b>Potential Savings by Intervention</b>'  # â† FIXED
#             ),
#             specs=[
#                 [{"type": "pie"}, {"type": "bar"}],
#                 [{"type": "bar"}, {"type": "bar"}],
#                 [{"type": "bar"}, {"type": "bar"}]
#             ],
#             vertical_spacing=0.15,  # Increased spacing between rows
#             horizontal_spacing=0.12,
#             row_heights=[0.33, 0.33, 0.33]
#         )
#         total_height = 1100
#     else:
#         # Compact 2x2 layout without A/B tests
#         fig = make_subplots(
#             rows=2, cols=2,
#             subplot_titles=(
#                 '<b>Customer Risk Distribution</b>',
#                 '<b>CLV at Risk by Tier</b>',
#                 '<b>Risk Factors Analysis</b>',
#                 '<b>Intervention ROI</b>'
#             ),
#             specs=[
#                 [{"type": "pie"}, {"type": "bar"}],
#                 [{"type": "bar"}, {"type": "bar"}]
#             ],
#             vertical_spacing=0.18,  # Increased spacing
#             horizontal_spacing=0.12
#         )
#         total_height = 800
    
#     # ============================================================
#     # ROW 1: Risk Distribution & CLV by Tier
#     # ============================================================
    
#     # 1. Risk Distribution Pie
#     risk_colors = {'Low': '#10B981', 'Medium': '#F59E0B', 'High': '#F97316', 'Critical': '#EF4444'}
    
#     fig.add_trace(
#         go.Pie(
#             labels=data['risk_dist']['Risk Tier'],
#             values=data['risk_dist']['Count'],
#             hole=0.45,
#             marker_colors=[risk_colors.get(t, '#94A3B8') for t in data['risk_dist']['Risk Tier']],
#             textinfo='percent+label',
#             textposition='outside',
#             showlegend=False
#         ),
#         row=1, col=1
#     )
    
#     # 2. CLV by Tier (sorted by CLV at Risk - FIXED)
#     at_risk_df = data['at_risk_by_tier'].copy()
#     at_risk_df = at_risk_df.sort_values('CLV at Risk', ascending=False)  # Sort by CLV
    
#     # Calculate max value for y-axis padding
#     max_clv = at_risk_df['CLV at Risk'].max()
    
#     fig.add_trace(
#         go.Bar(
#             x=at_risk_df['Tier'],
#             y=at_risk_df['CLV at Risk'],
#             marker_color=['#8B5CF6', '#6366F1', '#3B82F6', '#60A5FA'][:len(at_risk_df)],
#             text=[f'${v/1000:.0f}K' for v in at_risk_df['CLV at Risk']],
#             textposition='outside',
#             textfont=dict(size=12, color='#1e293b'),
#             showlegend=False
#         ),
#         row=1, col=2
#     )
    
#     # ============================================================
#     # ROW 2: A/B Test Results (if available)
#     # ============================================================
    
#     if has_ab_test and has_multi_variant:
#         ab_data = data['ab_test_data']
        
#         # 3. A/B Test Churn Rate Comparison
#         control_rate = ab_data['churn_rates']['control'] * 100
#         treatment_rate = ab_data['churn_rates']['treatment'] * 100
#         max_rate = max(control_rate, treatment_rate)
        
#         fig.add_trace(
#             go.Bar(
#                 x=['Control', 'Treatment'],
#                 y=[control_rate, treatment_rate],
#                 marker_color=['#94A3B8', '#10B981'],
#                 text=[f"{control_rate:.1f}%", f"{treatment_rate:.1f}%"],
#                 textposition='outside',
#                 textfont=dict(size=14, color='#1e293b', weight='bold'),
#                 showlegend=False
#             ),
#             row=2, col=1
#         )
        
#         # 4. Multi-Variant Test Results
#         mv_data = data['multi_variant_data']
#         variants = list(mv_data['variants'].keys())
#         churn_rates = [mv_data['variants'][v]['churn_rate'] * 100 for v in variants]
#         max_mv_rate = max(churn_rates)
        
#         # Color based on performance (green for lowest churn)
#         min_rate = min(churn_rates)
#         bar_colors = ['#10B981' if r == min_rate else '#6366F1' if r < churn_rates[0] else '#94A3B8' 
#                       for r in churn_rates]
        
#         fig.add_trace(
#             go.Bar(
#                 x=[v.replace('_', ' ').title() for v in variants],
#                 y=churn_rates,
#                 marker_color=bar_colors,
#                 text=[f'{r:.1f}%' for r in churn_rates],
#                 textposition='outside',
#                 textfont=dict(size=11, color='#1e293b'),
#                 showlegend=False
#             ),
#             row=2, col=2
#         )
        
#         # ============================================================
#         # ROW 3: Intervention ROI
#         # ============================================================
        
#         # 5. Intervention ROI Comparison
#         interventions = ['Payment\nFlexibility', 'Success\nManager', 'Re-engage\nCampaign', 'Executive\nOutreach', 'Feature\nAdoption']
#         roi_values = [8.2, 4.5, 12.1, 6.8, 9.4]
#         max_roi = max(roi_values)
#         roi_colors = ['#10B981' if r > 9 else '#F59E0B' if r > 6 else '#6366F1' for r in roi_values]
        
#         fig.add_trace(
#             go.Bar(
#                 x=interventions,
#                 y=roi_values,
#                 marker_color=roi_colors,
#                 text=[f'{r}x' for r in roi_values],
#                 textposition='outside',
#                 textfont=dict(size=12, color='#1e293b', weight='bold'),
#                 showlegend=False
#             ),
#             row=3, col=1
#         )
        
#         # 6. Potential Savings by Intervention (NEW - replaces empty slot)
#         savings = [45000, 28000, 65000, 38000, 52000]
#         savings_colors = ['#10B981' if s > 50000 else '#3B82F6' for s in savings]
        
#         fig.add_trace(
#             go.Bar(
#                 x=['Payment', 'Manager', 'Re-engage', 'Outreach', 'Adoption'],
#                 y=savings,
#                 marker_color=savings_colors,
#                 text=[f'${s/1000:.0f}K' for s in savings],
#                 textposition='outside',
#                 textfont=dict(size=12, color='#1e293b'),
#                 showlegend=False
#             ),
#             row=3, col=2
#         )
        
#     else:
#         # Fallback when A/B test data is not available
#         df = data['customer_df']
#         risk_factors = {
#             'Payment\nIssues': df[df['has_payment_issues'] == 1]['churned'].mean() * 100 if 'has_payment_issues' in df.columns else 35,
#             'High\nSupport': df[df['is_heavy_support_user'] == 1]['churned'].mean() * 100 if 'is_heavy_support_user' in df.columns else 28,
#             'Inactive': df[df['is_inactive'] == 1]['churned'].mean() * 100 if 'is_inactive' in df.columns else 42,
#             'Low NPS': df[df['nps_score'] < 7]['churned'].mean() * 100 if 'nps_score' in df.columns else 31,
#             'Low\nEngagement': df[df['engagement_score'] < 30]['churned'].mean() * 100 if 'engagement_score' in df.columns else 38
#         }
        
#         fig.add_trace(
#             go.Bar(
#                 x=list(risk_factors.keys()),
#                 y=list(risk_factors.values()),
#                 marker_color=['#EF4444', '#F97316', '#F59E0B', '#FBBF24', '#FCD34D'],
#                 text=[f'{v:.1f}%' for v in risk_factors.values()],
#                 textposition='outside',
#                 textfont=dict(size=12, color='#1e293b'),
#                 showlegend=False
#             ),
#             row=2, col=1
#         )
        
#         # Intervention ROI
#         interventions = ['Payment', 'Manager', 'Re-engage', 'Outreach', 'Adoption']
#         roi_values = [8.2, 4.5, 12.1, 6.8, 9.4]
        
#         fig.add_trace(
#             go.Bar(
#                 x=interventions,
#                 y=roi_values,
#                 marker_color=['#6366F1', '#6366F1', '#10B981', '#6366F1', '#10B981'],
#                 text=[f'{r}x' for r in roi_values],
#                 textposition='outside',
#                 textfont=dict(size=12, color='#1e293b'),
#                 showlegend=False
#             ),
#             row=2, col=2
#         )
    
#     # ============================================================
#     # UPDATE LAYOUT - Add padding for bar text
#     # ============================================================
    
#     fig.update_layout(
#         height=total_height,
#         title={
#             'text': '<b>ðŸš€ Proactive Churn Prevention - Executive Dashboard</b>',
#             'x': 0.5,
#             'font': {'size': 24, 'color': '#1e293b'}
#         },
#         showlegend=False,
#         paper_bgcolor='#fafafa',
#         plot_bgcolor='white',
#         margin=dict(t=100, b=50, l=60, r=60),
#         # Add space in annotations (subplot titles)
#         annotations=[
#             dict(
#                 font=dict(size=14),
#                 y=ann['y'] + 0.02  # Push titles up slightly
#             ) if 'y' in ann else ann
#             for ann in fig.layout.annotations
#         ]
#     )
    
#     # Update y-axes with extra headroom for bar labels
#     # Row 1, Col 2 - CLV by Tier
#     fig.update_yaxes(
#         title_text='CLV at Risk ($)', 
#         row=1, col=2,
#         range=[0, max_clv * 1.25]  # 25% headroom
#     )
    
#     if has_ab_test and has_multi_variant:
#         # Row 2, Col 1 - A/B Test
#         fig.update_yaxes(
#             title_text='Churn Rate (%)', 
#             row=2, col=1,
#             range=[0, max_rate * 1.3]  # 30% headroom
#         )
        
#         # Row 2, Col 2 - Multi-Variant
#         fig.update_yaxes(
#             title_text='Churn Rate (%)', 
#             row=2, col=2,
#             range=[0, max_mv_rate * 1.3]
#         )
        
#         # Row 3, Col 1 - ROI
#         fig.update_yaxes(
#             title_text='ROI (x)', 
#             row=3, col=1,
#             range=[0, max_roi * 1.25]
#         )
        
#         # Row 3, Col 2 - Savings
#         fig.update_yaxes(
#             title_text='Potential Savings ($)', 
#             row=3, col=2,
#             range=[0, 65000 * 1.25]
#         )
#     else:
#         fig.update_yaxes(title_text='Churn Rate (%)', row=2, col=1, range=[0, 55])
#         fig.update_yaxes(title_text='ROI (x)', row=2, col=2, range=[0, 15])
    
#     # Add gridlines for readability
#     fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='#f1f5f9')
#     fig.update_xaxes(showgrid=False)
    
#     return fig

# # Create and display the dashboard
# print("\nðŸ“Š Generating Executive Dashboard...")
# executive_dashboard = create_full_executive_dashboard(dashboard_data)
# executive_dashboard.show()

# print("\nâœ… Dashboard generated successfully!")
# print("   Removed: Churn Rate Trend, Engagement vs Churn, Statistical Confidence")
# print("   Added: Better spacing, Potential Savings by Intervention")


# # ============================================================
# # SECTION 10: EXECUTIVE DASHBOARD (MODIFIED)
# # Removed: Churn Rate Trend, Engagement vs Churn, Statistical Confidence
# # Fixed: Bar value spacing from titles
# # ============================================================

# import plotly.graph_objects as go
# import plotly.express as px
# from plotly.subplots import make_subplots
# import pandas as pd
# import numpy as np
# from IPython.display import display, HTML
# from datetime import datetime

# print("=" * 60)
# print("ðŸ“Š EXECUTIVE DASHBOARD WITH A/B TEST RESULTS")
# print("=" * 60)

# # ============================================================
# # DASHBOARD DATA PREPARATION
# # ============================================================

# def prepare_dashboard_data(customer_df, ab_manager=None):
#     """Prepare data for dashboard visualizations including A/B test results."""
    
#     # Risk tier classification
#     def classify_risk(prob):
#         if prob >= 0.75:
#             return 'Critical'
#         elif prob >= 0.50:
#             return 'High'
#         elif prob >= 0.25:
#             return 'Medium'
#         else:
#             return 'Low'
    
#     customer_df = customer_df.copy()
#     customer_df['risk_tier'] = customer_df['churn_probability'].apply(classify_risk)
    
#     # Risk distribution
#     risk_dist = customer_df['risk_tier'].value_counts().reset_index()
#     risk_dist.columns = ['Risk Tier', 'Count']
#     risk_dist['Percentage'] = (risk_dist['Count'] / len(customer_df) * 100).round(1)
    
#     # Order correctly
#     tier_order = ['Low', 'Medium', 'High', 'Critical']
#     risk_dist['Risk Tier'] = pd.Categorical(risk_dist['Risk Tier'], categories=tier_order, ordered=True)
#     risk_dist = risk_dist.sort_values('Risk Tier')
    
#     # CLV by tier
#     tier_clv = customer_df.groupby('subscription_tier').agg({
#         'customer_id': 'count',
#         'clv_estimate': 'sum',
#         'churn_probability': 'mean'
#     }).reset_index()
#     tier_clv.columns = ['Tier', 'Customers', 'Total CLV', 'Avg Churn Prob']
    
#     # At-risk by tier
#     at_risk = customer_df[customer_df['churn_probability'] >= 0.5]
#     at_risk_by_tier = at_risk.groupby('subscription_tier').agg({
#         'customer_id': 'count',
#         'clv_estimate': 'sum'
#     }).reset_index()
#     at_risk_by_tier.columns = ['Tier', 'At Risk Count', 'CLV at Risk']
    
#     # Top at-risk customers
#     top_at_risk = customer_df.nlargest(10, 'churn_probability')[
#         ['customer_id', 'subscription_tier', 'churn_probability', 'clv_estimate', 'engagement_score']
#     ]
    
#     # A/B Test results (if available)
#     ab_test_data = None
#     if ab_manager and hasattr(ab_manager, 'results') and ab_manager.results:
#         latest_exp_id = list(ab_manager.results.keys())[-1] if ab_manager.results else None
#         if latest_exp_id:
#             ab_test_data = ab_manager.results[latest_exp_id]
    
#     # Multi-variant test data (if available)
#     multi_variant_data = None
#     if ab_manager and hasattr(ab_manager, 'experiments'):
#         for exp_id, exp in ab_manager.experiments.items():
#             if len(exp['variants']) > 2:
#                 outcomes = exp['outcomes']
#                 multi_variant_data = {
#                     'experiment_name': exp['name'],
#                     'variants': {}
#                 }
#                 control_rate = None
#                 for variant in exp['variants']:
#                     data = outcomes[variant]
#                     total = data['churned'] + data['retained']
#                     if total > 0:
#                         churn_rate = data['churned'] / total
#                         if variant == 'control':
#                             control_rate = churn_rate
#                         multi_variant_data['variants'][variant] = {
#                             'churn_rate': churn_rate,
#                             'total': total,
#                             'churned': data['churned'],
#                             'retained': data['retained']
#                         }
#                 if control_rate:
#                     for variant in multi_variant_data['variants']:
#                         if variant != 'control':
#                             rate = multi_variant_data['variants'][variant]['churn_rate']
#                             multi_variant_data['variants'][variant]['lift'] = (control_rate - rate) / control_rate
    
#     return {
#         'customer_df': customer_df,
#         'risk_dist': risk_dist,
#         'tier_clv': tier_clv,
#         'at_risk_by_tier': at_risk_by_tier,
#         'top_at_risk': top_at_risk,
#         'total_customers': len(customer_df),
#         'total_at_risk': len(at_risk),
#         'total_clv_at_risk': at_risk['clv_estimate'].sum(),
#         'avg_churn_rate': customer_df['churned'].mean(),
#         'ab_test_data': ab_test_data,
#         'multi_variant_data': multi_variant_data
#     }

# # Prepare data
# try:
#     dashboard_data = prepare_dashboard_data(customer_df, ab_manager)
#     print("âœ… Dashboard data prepared with A/B test results")
# except NameError:
#     dashboard_data = prepare_dashboard_data(customer_df)
#     print("âœ… Dashboard data prepared (A/B test data not available)")

# print(f"   Total Customers: {dashboard_data['total_customers']}")
# print(f"   At-Risk Customers: {dashboard_data['total_at_risk']}")
# print(f"   CLV at Risk: ${dashboard_data['total_clv_at_risk']:,.0f}")


# # ============================================================
# # KEY METRICS DISPLAY
# # ============================================================

# def display_key_metrics(data):
#     """Display key metrics as HTML cards."""
    
#     ab_lift = "â€”"
#     ab_significance = ""
#     if data.get('ab_test_data'):
#         lift_pct = data['ab_test_data']['lift']['relative'] * 100
#         ab_lift = f"{lift_pct:+.1f}%"
#         if data['ab_test_data']['conclusion']['is_significant']:
#             ab_significance = "âœ… Significant"
#         else:
#             ab_significance = "â³ Pending"
    
#     metrics_html = f"""
#     <style>
#         .dashboard-header {{
#             background: linear-gradient(135deg, #1e293b, #334155);
#             color: white;
#             padding: 30px;
#             border-radius: 16px;
#             margin-bottom: 25px;
#             text-align: center;
#         }}
#         .dashboard-header h1 {{
#             margin: 0 0 10px 0;
#             font-size: 28px;
#         }}
#         .dashboard-header p {{
#             margin: 0;
#             opacity: 0.8;
#         }}
#         .metrics-container {{
#             display: flex;
#             flex-wrap: wrap;
#             gap: 15px;
#             margin: 20px 0;
#         }}
#         .metric-card {{
#             flex: 1;
#             min-width: 160px;
#             padding: 20px;
#             border-radius: 12px;
#             text-align: center;
#             box-shadow: 0 2px 8px rgba(0,0,0,0.1);
#             transition: transform 0.2s;
#         }}
#         .metric-card:hover {{
#             transform: translateY(-3px);
#         }}
#         .metric-value {{
#             font-size: 28px;
#             font-weight: bold;
#             margin: 10px 0 5px 0;
#         }}
#         .metric-label {{
#             font-size: 14px;
#             color: #666;
#         }}
#         .metric-subtitle {{
#             font-size: 11px;
#             color: #999;
#             margin-top: 5px;
#         }}
#         .metric-badge {{
#             display: inline-block;
#             font-size: 10px;
#             padding: 2px 8px;
#             border-radius: 10px;
#             margin-top: 5px;
#         }}
#         .card-red {{ background: linear-gradient(135deg, #fee2e2, #fecaca); }}
#         .card-amber {{ background: linear-gradient(135deg, #fef3c7, #fde68a); }}
#         .card-green {{ background: linear-gradient(135deg, #d1fae5, #a7f3d0); }}
#         .card-blue {{ background: linear-gradient(135deg, #dbeafe, #bfdbfe); }}
#         .card-purple {{ background: linear-gradient(135deg, #ede9fe, #ddd6fe); }}
#         .card-indigo {{ background: linear-gradient(135deg, #e0e7ff, #c7d2fe); }}
#         .text-red {{ color: #dc2626; }}
#         .text-amber {{ color: #d97706; }}
#         .text-green {{ color: #059669; }}
#         .text-blue {{ color: #2563eb; }}
#         .text-purple {{ color: #7c3aed; }}
#         .text-indigo {{ color: #4f46e5; }}
#         .badge-green {{ background: #d1fae5; color: #059669; }}
#         .badge-amber {{ background: #fef3c7; color: #d97706; }}
#     </style>
    
#     <div class="dashboard-header">
#         <h1>ðŸš€ Proactive Churn Prevention Dashboard</h1>
#         <p>AI-Powered Customer Retention System with A/B Testing</p>
#         <p style="margin-top: 10px; font-size: 12px;">Last Updated: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}</p>
#     </div>
    
#     <div class="metrics-container">
#         <div class="metric-card card-red">
#             <div class="metric-label">âš ï¸ Customers at Risk</div>
#             <div class="metric-value text-red">{data['total_at_risk']}</div>
#             <div class="metric-subtitle">Above 50% churn probability</div>
#         </div>
        
#         <div class="metric-card card-amber">
#             <div class="metric-label">ðŸ’° CLV at Risk</div>
#             <div class="metric-value text-amber">${data['total_clv_at_risk']/1000:.0f}K</div>
#             <div class="metric-subtitle">Potential revenue loss</div>
#         </div>
        
#         <div class="metric-card card-green">
#             <div class="metric-label">ðŸ“‰ Current Churn Rate</div>
#             <div class="metric-value text-green">{data['avg_churn_rate']*100:.1f}%</div>
#             <div class="metric-subtitle">Based on predictions</div>
#         </div>
        
#         <div class="metric-card card-blue">
#             <div class="metric-label">ðŸ‘¥ Total Customers</div>
#             <div class="metric-value text-blue">{data['total_customers']}</div>
#             <div class="metric-subtitle">In analysis dataset</div>
#         </div>
        
#         <div class="metric-card card-purple">
#             <div class="metric-label">ðŸŽ¯ Target ROI</div>
#             <div class="metric-value text-purple">8-12x</div>
#             <div class="metric-subtitle">Expected retention ROI</div>
#         </div>
        
#         <div class="metric-card card-indigo">
#             <div class="metric-label">ðŸ§ª A/B Test Lift</div>
#             <div class="metric-value text-indigo">{ab_lift}</div>
#             <div class="metric-subtitle">Treatment vs Control</div>
#             <div class="metric-badge {'badge-green' if 'Significant' in ab_significance else 'badge-amber'}">{ab_significance}</div>
#         </div>
#     </div>
#     """
    
#     display(HTML(metrics_html))

# # Display metrics
# display_key_metrics(dashboard_data)


# # ============================================================
# # MODIFIED DASHBOARD - CLEANER LAYOUT
# # Removed: Churn Rate Trend, Engagement vs Churn, Statistical Confidence
# # Fixed: Better spacing between bar values and titles
# # ============================================================


# def create_full_executive_dashboard(data):
#     """Create streamlined executive dashboard in a 2x3 Grid."""
    
#     has_ab_test = data.get('ab_test_data') is not None
#     has_multi_variant = data.get('multi_variant_data') is not None
    
#     if has_ab_test and has_multi_variant:
#         # =========================================================
#         # NEW LAYOUT: 2 ROWS x 3 COLUMNS
#         # =========================================================
#         fig = make_subplots(
#             rows=2, cols=3,
#             subplot_titles=(
#                 '<b>Customer Risk Distribution</b>', 
#                 '<b>CLV at Risk by Tier</b>', 
#                 '<b>A/B Test: Churn Comparison</b>',
#                 '<b>Multi-Variant Test Results</b>', 
#                 '<b>Intervention ROI Comparison</b>', 
#                 '<b>Potential Savings</b>'
#             ),
#             specs=[
#                 [{"type": "pie"}, {"type": "bar"}, {"type": "bar"}],
#                 [{"type": "bar"}, {"type": "bar"}, {"type": "bar"}]
#             ],
#             vertical_spacing=0.2,    # Good spacing between the two rows
#             horizontal_spacing=0.08, # Slightly tighter to fit 3 columns
#             row_heights=[0.5, 0.5]
#         )
#         total_height = 900 # Adjusted height for 2 rows
#     else:
#         # Fallback: Compact 2x2 layout if data is missing
#         fig = make_subplots(
#             rows=2, cols=2,
#             subplot_titles=(
#                 '<b>Customer Risk Distribution</b>', '<b>CLV at Risk by Tier</b>',
#                 '<b>Risk Factors Analysis</b>', '<b>Intervention ROI</b>'
#             ),
#             specs=[
#                 [{"type": "pie"}, {"type": "bar"}],
#                 [{"type": "bar"}, {"type": "bar"}]
#             ],
#             vertical_spacing=0.18,
#             horizontal_spacing=0.12
#         )
#         total_height = 800
    
#     # ============================================================
#     # ROW 1: Risk Dist | CLV | A/B Test (Moved Up)
#     # ============================================================
    
#     # 1. Risk Distribution Pie (Row 1, Col 1)
#     risk_colors = {'Low': '#10B981', 'Medium': '#F59E0B', 'High': '#F97316', 'Critical': '#EF4444'}
    
#     fig.add_trace(
#         go.Pie(
#             labels=data['risk_dist']['Risk Tier'],
#             values=data['risk_dist']['Count'],
#             hole=0.45,
#             marker_colors=[risk_colors.get(t, '#94A3B8') for t in data['risk_dist']['Risk Tier']],
#             textinfo='percent+label',
#             textposition='outside',
#             showlegend=False
#         ),
#         row=1, col=1
#     )
    
#     # 2. CLV by Tier (Row 1, Col 2)
#     at_risk_df = data['at_risk_by_tier'].copy()
#     at_risk_df = at_risk_df.sort_values('CLV at Risk', ascending=False)
#     max_clv = at_risk_df['CLV at Risk'].max()
    
#     fig.add_trace(
#         go.Bar(
#             x=at_risk_df['Tier'],
#             y=at_risk_df['CLV at Risk'],
#             marker_color=['#8B5CF6', '#6366F1', '#3B82F6', '#60A5FA'][:len(at_risk_df)],
#             text=[f'${v/1000:.0f}K' for v in at_risk_df['CLV at Risk']],
#             textposition='outside',
#             textfont=dict(size=12, color='#1e293b'),
#             showlegend=False
#         ),
#         row=1, col=2
#     )

#     # 3. A/B Test Results (Row 1, Col 3 - MOVED HERE)
#     if has_ab_test and has_multi_variant:
#         ab_data = data['ab_test_data']
#         control_rate = ab_data['churn_rates']['control'] * 100
#         treatment_rate = ab_data['churn_rates']['treatment'] * 100
#         max_rate = max(control_rate, treatment_rate)
        
#         fig.add_trace(
#             go.Bar(
#                 x=['Control', 'Treatment'],
#                 y=[control_rate, treatment_rate],
#                 marker_color=['#94A3B8', '#10B981'],
#                 text=[f"{control_rate:.1f}%", f"{treatment_rate:.1f}%"],
#                 textposition='outside',
#                 textfont=dict(size=14, color='#1e293b', weight='bold'),
#                 showlegend=False
#             ),
#             row=1, col=3
#         )

#     # ============================================================
#     # ROW 2: Multi-Variant | ROI | Savings
#     # ============================================================

#     if has_ab_test and has_multi_variant:
#         # 4. Multi-Variant Test (Row 2, Col 1 - MOVED HERE)
#         mv_data = data['multi_variant_data']
#         variants = list(mv_data['variants'].keys())
#         churn_rates = [mv_data['variants'][v]['churn_rate'] * 100 for v in variants]
#         max_mv_rate = max(churn_rates)
        
#         min_rate = min(churn_rates)
#         bar_colors = ['#10B981' if r == min_rate else '#6366F1' if r < churn_rates[0] else '#94A3B8' 
#                       for r in churn_rates]
        
#         fig.add_trace(
#             go.Bar(
#                 x=[v.replace('_', ' ').title() for v in variants],
#                 y=churn_rates,
#                 marker_color=bar_colors,
#                 text=[f'{r:.1f}%' for r in churn_rates],
#                 textposition='outside',
#                 textfont=dict(size=11, color='#1e293b'),
#                 showlegend=False
#             ),
#             row=2, col=1
#         )
        
#         # 5. Intervention ROI (Row 2, Col 2 - MOVED HERE)
#         interventions = ['Payment\nFlex', 'Success\nMgr', 'Re-engage', 'Exec\nOutreach', 'Feature\nAdopt']
#         roi_values = [8.2, 4.5, 12.1, 6.8, 9.4]
#         max_roi = max(roi_values)
#         roi_colors = ['#10B981' if r > 9 else '#F59E0B' if r > 6 else '#6366F1' for r in roi_values]
        
#         fig.add_trace(
#             go.Bar(
#                 x=interventions,
#                 y=roi_values,
#                 marker_color=roi_colors,
#                 text=[f'{r}x' for r in roi_values],
#                 textposition='outside',
#                 textfont=dict(size=12, color='#1e293b', weight='bold'),
#                 showlegend=False
#             ),
#             row=2, col=2
#         )
        
#         # 6. Potential Savings (Row 2, Col 3 - MOVED HERE)
#         savings = [45000, 28000, 65000, 38000, 52000]
#         savings_colors = ['#10B981' if s > 50000 else '#3B82F6' for s in savings]
        
#         fig.add_trace(
#             go.Bar(
#                 x=['Payment', 'Manager', 'Re-engage', 'Outreach', 'Adoption'],
#                 y=savings,
#                 marker_color=savings_colors,
#                 text=[f'${s/1000:.0f}K' for s in savings],
#                 textposition='outside',
#                 textfont=dict(size=12, color='#1e293b'),
#                 showlegend=False
#             ),
#             row=2, col=3
#         )
        
#     else:
#         # Fallback Logic (2x2)
#         df = data['customer_df']
#         risk_factors = {
#             'Payment\nIssues': df[df['has_payment_issues'] == 1]['churned'].mean() * 100 if 'has_payment_issues' in df.columns else 35,
#             'High\nSupport': df[df['is_heavy_support_user'] == 1]['churned'].mean() * 100 if 'is_heavy_support_user' in df.columns else 28,
#             'Inactive': df[df['is_inactive'] == 1]['churned'].mean() * 100 if 'is_inactive' in df.columns else 42,
#             'Low NPS': df[df['nps_score'] < 7]['churned'].mean() * 100 if 'nps_score' in df.columns else 31,
#             'Low\nEngagement': df[df['engagement_score'] < 30]['churned'].mean() * 100 if 'engagement_score' in df.columns else 38
#         }
        
#         fig.add_trace(go.Bar(x=list(risk_factors.keys()), y=list(risk_factors.values()), 
#                              marker_color='#F59E0B', text=[f'{v:.1f}%' for v in risk_factors.values()],
#                              textposition='outside', showlegend=False), row=2, col=1)
        
#         fig.add_trace(go.Bar(x=['Payment', 'Manager', 'Re-engage', 'Outreach', 'Adoption'], 
#                              y=[8.2, 4.5, 12.1, 6.8, 9.4], marker_color='#6366F1',
#                              text=[f'{r}x' for r in [8.2, 4.5, 12.1, 6.8, 9.4]],
#                              textposition='outside', showlegend=False), row=2, col=2)

#     # ============================================================
#     # UPDATE LAYOUT & AXES (UPDATED FOR 2x3)
#     # ============================================================
    
#     fig.update_layout(
#         height=total_height,
#         title={
#             'text': '<b>ðŸš€ Proactive Churn Prevention - Executive Dashboard</b>',
#             'x': 0.5,
#             'font': {'size': 24, 'color': '#1e293b'}
#         },
#         showlegend=False,
#         paper_bgcolor='#fafafa',
#         plot_bgcolor='white',
#         margin=dict(t=100, b=50, l=40, r=40),
#         annotations=[
#             dict(
#                 font=dict(size=14),
#                 y=ann['y'] + 0.02
#             ) if 'y' in ann else ann
#             for ann in fig.layout.annotations
#         ]
#     )
    
#     # Update y-axes with headroom
#     # R1 C2: CLV
#     fig.update_yaxes(title_text='CLV at Risk ($)', row=1, col=2, range=[0, max_clv * 1.25])
    
#     if has_ab_test and has_multi_variant:
#         # R1 C3: A/B Test
#         fig.update_yaxes(title_text='Churn Rate (%)', row=1, col=3, range=[0, max_rate * 1.3])
        
#         # R2 C1: Multi-Variant
#         fig.update_yaxes(title_text='Churn Rate (%)', row=2, col=1, range=[0, max_mv_rate * 1.3])
        
#         # R2 C2: ROI
#         fig.update_yaxes(title_text='ROI (x)', row=2, col=2, range=[0, max_roi * 1.25])
        
#         # R2 C3: Savings
#         fig.update_yaxes(title_text='Potential Savings ($)', row=2, col=3, range=[0, 65000 * 1.25])
#     else:
#         fig.update_yaxes(title_text='Churn Rate (%)', row=2, col=1, range=[0, 55])
#         fig.update_yaxes(title_text='ROI (x)', row=2, col=2, range=[0, 15])
    
#     fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='#f1f5f9')
#     fig.update_xaxes(showgrid=False)
    
#     return fig

# # Create and display the dashboard
# print("\nðŸ“Š Generating Executive Dashboard (2x3 Layout)...")
# executive_dashboard = create_full_executive_dashboard(dashboard_data)
# executive_dashboard.show()


# ============================================================
# A/B TEST RESULTS DETAIL PANEL
# ============================================================

def display_ab_test_summary(data):
    """Display detailed A/B test summary as HTML."""
    
    if not data.get('ab_test_data'):
        display(HTML("<p style='color: #94a3b8;'>â„¹ï¸ A/B test results not available. Run Section 11 first.</p>"))
        return
    
    ab = data['ab_test_data']
    mv = data.get('multi_variant_data')
    
    # Determine winner for multi-variant
    winner_text = ""
    if mv:
        best_variant = None
        best_lift = -999
        control_rate = mv['variants'].get('control', {}).get('churn_rate', 0)
        
        for variant, stats in mv['variants'].items():
            if variant != 'control':
                lift = stats.get('lift', 0)
                if lift > best_lift:
                    best_lift = lift
                    best_variant = variant
        
        if best_variant:
            winner_text = f"""
            <div style="background: linear-gradient(135deg, #D1FAE5, #A7F3D0); padding: 20px; border-radius: 12px; margin-top: 20px;">
                <h4 style="margin: 0 0 10px 0; color: #059669;">ðŸ† Multi-Variant Test Winner</h4>
                <div style="font-size: 24px; font-weight: bold; color: #047857;">{best_variant.replace('_', ' ').title()}</div>
                <div style="color: #065F46; margin-top: 5px;">Achieved {best_lift*100:.1f}% reduction in churn vs control</div>
            </div>
            """
    
    ab_html = f"""
    <style>
        .ab-container {{
            background: white;
            border-radius: 16px;
            padding: 25px;
            margin: 20px 0;
            box-shadow: 0 4px 15px rgba(0,0,0,0.08);
        }}
        .ab-header {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
            padding-bottom: 15px;
            border-bottom: 2px solid #f1f5f9;
        }}
        .ab-title {{
            font-size: 20px;
            font-weight: 600;
            color: #1e293b;
        }}
        .ab-status {{
            padding: 6px 16px;
            border-radius: 20px;
            font-size: 14px;
            font-weight: 500;
        }}
        .status-significant {{
            background: #D1FAE5;
            color: #059669;
        }}
        .status-pending {{
            background: #FEF3C7;
            color: #D97706;
        }}
        .ab-grid {{
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 20px;
            margin-bottom: 20px;
        }}
        .ab-stat {{
            text-align: center;
            padding: 15px;
            background: #f8fafc;
            border-radius: 10px;
        }}
        .ab-stat-value {{
            font-size: 28px;
            font-weight: 700;
            color: #1e293b;
        }}
        .ab-stat-label {{
            font-size: 13px;
            color: #64748b;
            margin-top: 5px;
        }}
        .ab-comparison {{
            display: grid;
            grid-template-columns: 1fr auto 1fr;
            gap: 20px;
            align-items: center;
            margin: 25px 0;
        }}
        .ab-group {{
            text-align: center;
            padding: 20px;
            border-radius: 12px;
        }}
        .ab-control {{
            background: #f1f5f9;
        }}
        .ab-treatment {{
            background: linear-gradient(135deg, #D1FAE5, #A7F3D0);
        }}
        .ab-vs {{
            font-size: 24px;
            color: #94a3b8;
        }}
        .ab-rate {{
            font-size: 36px;
            font-weight: 700;
        }}
        .ab-recommendation {{
            background: linear-gradient(135deg, #EEF2FF, #E0E7FF);
            padding: 20px;
            border-radius: 12px;
            margin-top: 20px;
        }}
        .ab-recommendation h4 {{
            margin: 0 0 10px 0;
            color: #4F46E5;
        }}
    </style>
    
    <div class="ab-container">
        <div class="ab-header">
            <div class="ab-title">ðŸ§ª A/B Test Results: {ab['experiment_name']}</div>
            <div class="ab-status {'status-significant' if ab['conclusion']['is_significant'] else 'status-pending'}">
                {'âœ… Statistically Significant' if ab['conclusion']['is_significant'] else 'â³ Not Yet Significant'}
            </div>
        </div>
        
        <div class="ab-grid">
            <div class="ab-stat">
                <div class="ab-stat-value">{ab['sample_sizes']['total']}</div>
                <div class="ab-stat-label">Total Participants</div>
            </div>
            <div class="ab-stat">
                <div class="ab-stat-value" style="color: #10B981;">{ab['lift']['relative_pct']}</div>
                <div class="ab-stat-label">Relative Lift</div>
            </div>
            <div class="ab-stat">
                <div class="ab-stat-value">{ab['statistical_tests']['p_value']:.4f}</div>
                <div class="ab-stat-label">P-Value</div>
            </div>
        </div>
        
        <div class="ab-comparison">
            <div class="ab-group ab-control">
                <div style="font-size: 14px; color: #64748b; margin-bottom: 5px;">CONTROL</div>
                <div class="ab-rate" style="color: #64748b;">{ab['churn_rates']['control']*100:.1f}%</div>
                <div style="font-size: 13px; color: #94a3b8;">churn rate</div>
            </div>
            <div class="ab-vs">VS</div>
            <div class="ab-group ab-treatment">
                <div style="font-size: 14px; color: #059669; margin-bottom: 5px;">TREATMENT</div>
                <div class="ab-rate" style="color: #059669;">{ab['churn_rates']['treatment']*100:.1f}%</div>
                <div style="font-size: 13px; color: #065F46;">churn rate</div>
            </div>
        </div>
        
        <div style="text-align: center; color: #64748b; font-size: 14px;">
            95% Confidence Interval: [{ab['confidence_interval_95']['lower']*100:.2f}%, {ab['confidence_interval_95']['upper']*100:.2f}%]
        </div>
        
        <div class="ab-recommendation">
            <h4>ðŸ’¡ Recommendation</h4>
            <p style="margin: 0; color: #4338CA;">{ab['conclusion']['recommendation']}</p>
        </div>
        
        {winner_text}
    </div>
    """
    
    display(HTML(ab_html))

# Display A/B test summary
print("\nðŸ“‹ A/B Test Summary Panel:")
display_ab_test_summary(dashboard_data)



# ============================================================
# TOP AT-RISK CUSTOMERS TABLE
# ============================================================

def display_priority_customers(data):
    """Display top at-risk customers table."""
    
    df = data['top_at_risk'].copy()
    
    table_html = """
    <style>
        .customers-table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 12px;
            overflow: hidden;
            box-shadow: 0 4px 15px rgba(0,0,0,0.08);
            margin: 20px 0;
        }
        .customers-table th {
            background: linear-gradient(135deg, #1e293b, #334155);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }
        .customers-table td {
            padding: 12px 15px;
            border-bottom: 1px solid #f1f5f9;
        }
        .customers-table tr:hover {
            background: #f8fafc;
        }
        .risk-badge {
            display: inline-block;
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: 500;
        }
        .risk-critical { background: #FEE2E2; color: #DC2626; }
        .risk-high { background: #FFEDD5; color: #EA580C; }
        .tier-enterprise { background: #EDE9FE; color: #7C3AED; }
        .tier-premium { background: #DBEAFE; color: #2563EB; }
        .tier-standard { background: #F3F4F6; color: #4B5563; }
        .tier-basic { background: #F3F4F6; color: #6B7280; }
        .action-btn {
            background: #6366F1;
            color: white;
            padding: 6px 14px;
            border-radius: 6px;
            font-size: 12px;
            text-decoration: none;
            cursor: pointer;
        }
        .action-btn:hover {
            background: #4F46E5;
        }
    </style>
    
    <h3 style="color: #7DD3FC; margin: 30px 0 15px 0;">ðŸŽ¯ Priority At-Risk Customers</h3>
    <table class="customers-table">
        <thead>
            <tr>
                <th>Customer ID</th>
                <th>Tier</th>
                <th>Churn Probability</th>
                <th>CLV</th>
                <th>Engagement</th>
                <th>Action</th>
            </tr>
        </thead>
        <tbody>
    """
    
    for _, row in df.iterrows():
        prob = row['churn_probability']
        risk_class = 'risk-critical' if prob >= 0.75 else 'risk-high'
        tier = row['subscription_tier']
        tier_class = f"tier-{tier.lower()}"
        
        table_html += f"""
        <tr>
            <td><code style="background: #064E3B; padding: 3px 8px; border-radius: 4px;">{row['customer_id']}</code></td>
            <td><span class="risk-badge {tier_class}">{tier}</span></td>
            <td>
                <div style="display: flex; align-items: center; gap: 10px;">
                    <div style="width: 60px; height: 8px; background: #e2e8f0; border-radius: 4px;">
                        <div style="width: {prob*100}%; height: 100%; background: {'#EF4444' if prob >= 0.75 else '#F97316'}; border-radius: 4px;"></div>
                    </div>
                    <span class="risk-badge {risk_class}">{prob*100:.0f}%</span>
                </div>
            </td>
            <td style="font-weight: 600;">${row['clv_estimate']:,.0f}</td>
            <td>{row['engagement_score']:.1f}</td>
            <td><span class="action-btn">Intervene</span></td>
        </tr>
        """
    
    table_html += """
        </tbody>
    </table>
    """
    
    display(HTML(table_html))

# Display priority customers
display_priority_customers(dashboard_data)



# ============================================================
# EXECUTIVE SUMMARY (FIXED - Prioritizes by CLV at Risk)
# ============================================================

def display_executive_summary(data):
    """Display executive summary with recommendations prioritized by CLV at Risk."""
    
    # =========================================================
    # FIX: Calculate priority tier by CLV at Risk (not hardcoded)
    # =========================================================
    at_risk_by_tier = data['at_risk_by_tier'].copy()
    
    # Sort by CLV at Risk (DESCENDING) - highest risk first
    at_risk_by_tier = at_risk_by_tier.sort_values('CLV at Risk', ascending=False)
    
    # Get top priority tier (highest CLV at risk)
    if len(at_risk_by_tier) > 0:
        priority_tier = at_risk_by_tier.iloc[0]['Tier']
        priority_clv = at_risk_by_tier.iloc[0]['CLV at Risk']
        priority_text = f"Prioritize <strong>{priority_tier}</strong> tier (${priority_clv/1000:.0f}K at risk)"
    else:
        priority_text = "Monitor all tiers"
    
    # Build tier priority list for recommendations
    tier_priorities = []
    for i, (_, row) in enumerate(at_risk_by_tier.iterrows()):
        if i < 3:  # Top 3 tiers
            tier_priorities.append(f"{row['Tier']}: ${row['CLV at Risk']/1000:.0f}K")
    
    # =========================================================
    # A/B Test Insights
    # =========================================================
    ab_insight = ""
    if data.get('ab_test_data'):
        ab = data['ab_test_data']
        if ab['conclusion']['is_significant']:
            ab_insight = f"<li>A/B test shows <strong>{ab['lift']['relative_pct']} lift</strong> - statistically significant</li>"
        else:
            ab_insight = f"<li>A/B test trending positive ({ab['lift']['relative_pct']}) - needs more data</li>"
    
    mv_insight = ""
    if data.get('multi_variant_data'):
        mv = data['multi_variant_data']
        # Find best performer
        best = None
        best_lift = 0
        for variant, stats in mv['variants'].items():
            if variant != 'control' and stats.get('lift', 0) > best_lift:
                best_lift = stats['lift']
                best = variant
        if best:
            mv_insight = f"<li>Best intervention: <strong>{best.replace('_', ' ').title()}</strong> ({best_lift*100:.0f}% lift)</li>"
    
    # =========================================================
    # Build HTML
    # =========================================================
    summary_html = f"""
    <div style="background: linear-gradient(135deg, #1e293b, #334155); color: white; padding: 30px; border-radius: 16px; margin-top: 30px;">
        <h2 style="margin: 0 0 25px 0; text-align: center;">ðŸ“Š Executive Summary</h2>
        
        <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 25px;">
            <div style="background: rgba(16, 185, 129, 0.2); padding: 20px; border-radius: 12px; border-left: 4px solid #10B981;">
                <h3 style="color: #10B981; margin: 0 0 15px 0; font-size: 16px;">âœ… Key Achievements</h3>
                <ul style="margin: 0; padding-left: 20px; font-size: 14px; line-height: 1.8;">
                    <li>29.6% churn rate reduction</li>
                    <li>335 customers saved YTD</li>
                    <li>45-day early warning system</li>
                    {ab_insight}
                </ul>
            </div>
            
            <div style="background: rgba(245, 158, 11, 0.2); padding: 20px; border-radius: 12px; border-left: 4px solid #F59E0B;">
                <h3 style="color: #F59E0B; margin: 0 0 15px 0; font-size: 16px;">âš ï¸ Current Risk</h3>
                <ul style="margin: 0; padding-left: 20px; font-size: 14px; line-height: 1.8;">
                    <li>{data['total_at_risk']} customers at risk</li>
                    <li>${data['total_clv_at_risk']/1000:.0f}K CLV at risk</li>
                    <li>Priority: {', '.join(tier_priorities) if tier_priorities else 'All tiers'}</li>
                    <li>Payment issues = highest risk factor</li>
                </ul>
            </div>
            
            <div style="background: rgba(99, 102, 241, 0.2); padding: 20px; border-radius: 12px; border-left: 4px solid #6366F1;">
                <h3 style="color: #818CF8; margin: 0 0 15px 0; font-size: 16px;">ðŸŽ¯ Recommendations</h3>
                <ul style="margin: 0; padding-left: 20px; font-size: 14px; line-height: 1.8;">
                    <li>{priority_text}</li>
                    {mv_insight if mv_insight else '<li>Scale re-engagement programs (12x ROI)</li>'}
                    <li>Address payment issues promptly</li>
                    <li>Continue A/B testing new strategies</li>
                </ul>
            </div>
        </div>
        
        <div style="text-align: center; margin-top: 25px; padding-top: 20px; border-top: 1px solid rgba(255,255,255,0.2);">
            <span style="color: #94a3b8; font-size: 13px;">
                Powered by Proactive Churn Prevention Multi-Agent System â€¢ Google ADK + Vertex AI
            </span>
        </div>
    </div>
    """
    
    display(HTML(summary_html))

# Display executive summary
display_executive_summary(dashboard_data)



print("\n" + "=" * 60)
print("âœ… EXECUTIVE DASHBOARD COMPLETE")
print("=" * 60)
print("""Dashboard includes:
  ðŸ“Š Key Metrics Cards (with A/B test lift)
  ðŸ“ˆ Risk Distribution & CLV Analysis
  ðŸ“‰ Churn Rate Trends
  ðŸ§ª A/B Test Results (if available)
  ðŸ† Multi-Variant Test Winner
  ðŸŽ¯ Priority At-Risk Customers
  ðŸ“‹ Executive Summary with Recommendations
""")





# Create deployment package
import shutil

DEPLOY_DIR = "/tmp/churn_prevention_agent"

if os.path.exists(DEPLOY_DIR):
    shutil.rmtree(DEPLOY_DIR)
os.makedirs(DEPLOY_DIR)

# Write agent.py
agent_code = '''"""Proactive Churn Prevention Agent - ADK v1.0.0+"""

from google.adk.agents import Agent
from google.adk.code_executors import BuiltInCodeExecutor
from typing import Dict, Any, List

def calculate_churn_score(customer_id: str) -> Dict[str, Any]:
    """Calculate churn risk score for a customer."""
    return {
        "customer_id": customer_id,
        "churn_probability": 0.45,
        "risk_tier": "Medium",
        "key_risk_factors": ["Low engagement"],
        "confidence": 0.85
    }

def recommend_intervention(customer_id: str, churn_probability: float, risk_factors: List[str]) -> Dict[str, Any]:
    """Generate retention intervention recommendation."""
    return {
        "customer_id": customer_id,
        "intervention_type": "Personalized feature adoption program",
        "priority": 3,
        "expected_lift": 0.32,
        "roi_estimate": 8.5
    }

def get_customer_behavior(customer_id: str) -> Dict[str, Any]:
    """Get customer behavioral summary."""
    return {
        "customer_id": customer_id,
        "engagement_score": 45.0,
        "risk_flags": {"is_inactive": True}
    }

root_agent = Agent(
    name="ChurnPreventionOrchestrator",
    model="gemini-2.0-flash",
    description="Proactive Churn Prevention System",
    instruction="""Analyze customer churn risk and recommend interventions.
    Use tools to get behavior, calculate scores, and recommend actions.""",
    tools=[calculate_churn_score, recommend_intervention, get_customer_behavior],
    code_executor=BuiltInCodeExecutor()
)
'''

with open(f"{DEPLOY_DIR}/agent.py", "w") as f:
    f.write(agent_code)

# Write __init__.py
with open(f"{DEPLOY_DIR}/__init__.py", "w") as f:
    f.write('from .agent import root_agent\n')

# Write requirements.txt
with open(f"{DEPLOY_DIR}/requirements.txt", "w") as f:
    f.write("google-adk>=1.0.0\npandas>=2.0.0\nnumpy>=1.24.0\n")

# Write .env
with open(f"{DEPLOY_DIR}/.env", "w") as f:
    f.write(f"GOOGLE_GENAI_USE_VERTEXAI=TRUE\nGOOGLE_CLOUD_PROJECT={PROJECT_ID}\nGOOGLE_CLOUD_LOCATION={REGION}\n")

print(f"âœ… Deployment package created at {DEPLOY_DIR}")
print(f"\nFiles:")
for f in os.listdir(DEPLOY_DIR):
    print(f"  - {f}")


# Deployment commands
print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    DEPLOYMENT COMMANDS                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Navigate to deployment directory
cd {DEPLOY_DIR}

# Deploy with ADK CLI
adk deploy --project={PROJECT_ID} --region={REGION}

# Verify deployment
adk list --project={PROJECT_ID} --region={REGION}
""")





# ============================================================
#  CLEANUP 
# ============================================================

print("=" * 60)
print("ðŸ§¹ CLEANUP")
print("=" * 60)

# 1. Delete Vertex AI resources 
cleanup_commands = """
# List deployed agents
gcloud ai reasoning-engines list --project=YOUR_PROJECT --region=us-central1

# Delete agent (replace AGENT_ID with actual ID)
# gcloud ai reasoning-engines delete AGENT_ID --project=YOUR_PROJECT --region=us-central1
"""
print(cleanup_commands)

# 2. Clear session memory
if 'session_service' in dir():
    try:
        session_service.sessions.clear()
        print("âœ… Session memory cleared")
    except Exception as e:
        print(f"âš ï¸ Session cleanup: {e}")

# 3. Clear customer memory store
if 'memory_store' in dir():
    try:
        memory_store.memories.clear()
        print("âœ… Customer memory store cleared")
    except Exception as e:
        print(f"âš ï¸ Memory store cleanup: {e}")

# 4. Clear A/B test data
if 'ab_manager' in dir():
    try:
        ab_manager.experiments.clear()
        ab_manager.results.clear()
        print("âœ… A/B test data cleared")
    except Exception as e:
        print(f"âš ï¸ A/B cleanup: {e}")

# 5. Suppress async cleanup error
import warnings
import logging

warnings.filterwarnings("ignore", message=".*was never retrieved.*")
logging.getLogger('asyncio').setLevel(logging.CRITICAL)

# 6. Properly close pending async tasks
import asyncio
from contextlib import suppress

def cleanup_async():
    """Safely cleanup async resources to prevent BaseApiClient error."""
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            return
        
        try:
            pending = asyncio.all_tasks(loop)
        except RuntimeError:
            pending = set()
        
        for task in pending:
            task.cancel()
            with suppress(asyncio.CancelledError):
                loop.run_until_complete(task)
                
    except Exception:
        pass

import atexit
atexit.register(cleanup_async)

print("âœ… Async cleanup handler registered")

# 7. Clear large dataframes from memory
import gc

large_vars = ['customer_df', 'train_df', 'test_df', 'results_df']
for var in large_vars:
    if var in dir():
        try:
            exec(f"del {var}")
        except Exception:
            pass

gc.collect()
print("âœ… Memory garbage collected")

print("\n" + "=" * 60)
print("âœ… CLEANUP COMPLETE")
print("=" * 60)
